{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1187d7fc",
   "metadata": {},
   "source": [
    "# Filtering the Noise: ML for Trustworthy Location Reviews\n",
    "## 24-Hour Hackathon Solution\n",
    "\n",
    "**Team:** [Your Team Name]  \n",
    "**Date:** August 27, 2025  \n",
    "**Challenge:** Design and implement an ML-based system to evaluate the quality and relevancy of Google location reviews\n",
    "\n",
    "### Problem Statement\n",
    "- **Gauge review quality**: Detect spam, advertisements, irrelevant content, and rants\n",
    "- **Assess relevancy**: Determine if review content is genuinely related to the location\n",
    "- **Enforce policies**: Automatically flag reviews violating predefined policies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3b4810",
   "metadata": {},
   "source": [
    "## 🔨 Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3fd6026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (4.56.0)\n",
      "Requirement already satisfied: torch in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (2.8.0)\n",
      "Requirement already satisfied: datasets in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (4.0.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (2.3.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (2.3.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (1.7.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (3.10.6)\n",
      "Requirement already satisfied: seaborn in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (0.13.2)\n",
      "Requirement already satisfied: plotly in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (6.3.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (3.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (0.34.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (2025.8.29)\n",
      "Requirement already satisfied: requests in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (0.22.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from datasets) (21.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: xxhash in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from scikit-learn) (1.16.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib) (4.59.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from plotly) (2.2.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from fsspec->torch) (3.12.15)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from requests->transformers) (2025.8.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->torch) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->torch) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->torch) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->torch) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->torch) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->torch) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->torch) (1.20.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: huggingface-hub in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (0.34.4)\n",
      "Requirement already satisfied: accelerate in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (1.10.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from huggingface-hub) (3.19.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from huggingface-hub) (2025.3.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from huggingface-hub) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from huggingface-hub) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from huggingface-hub) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from huggingface-hub) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from huggingface-hub) (4.15.0)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from accelerate) (2.3.2)\n",
      "Requirement already satisfied: psutil in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from accelerate) (2.8.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from accelerate) (0.6.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from torch>=2.0.0->accelerate) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: colorama in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from tqdm>=4.42.1->huggingface-hub) (0.4.6)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from requests->huggingface-hub) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from requests->huggingface-hub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from requests->huggingface-hub) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from requests->huggingface-hub) (2025.8.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (3.9.1)\n",
      "Requirement already satisfied: spacy in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (3.8.7)\n",
      "Requirement already satisfied: wordcloud in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (1.9.4)\n",
      "Requirement already satisfied: click in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from nltk) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from nltk) (2025.8.29)\n",
      "Requirement already satisfied: tqdm in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (1.0.13)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (3.0.10)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (8.3.6)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (0.17.3)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (2.3.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (2.32.5)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (2.11.7)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\program files\\python311\\lib\\site-packages (from spacy) (65.5.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (25.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: pillow in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from wordcloud) (11.3.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from wordcloud) (3.10.6)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.8.3)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from tqdm->nltk) (0.4.6)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (14.1.0)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.22.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.3.0.post1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from jinja2->spacy) (3.0.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib->wordcloud) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib->wordcloud) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib->wordcloud) (4.59.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib->wordcloud) (1.4.9)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib->wordcloud) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib->wordcloud) (2.9.0.post0)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.3.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.17.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
      "Requirement already satisfied: wrapt in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.3)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: kaggle in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (1.7.4.5)\n",
      "Requirement already satisfied: bleach in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from kaggle) (6.2.0)\n",
      "Requirement already satisfied: certifi>=14.05.14 in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from kaggle) (2025.8.3)\n",
      "Requirement already satisfied: charset-normalizer in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from kaggle) (3.4.3)\n",
      "Requirement already satisfied: idna in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from kaggle) (3.10)\n",
      "Requirement already satisfied: protobuf in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from kaggle) (6.32.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from kaggle) (2.9.0.post0)\n",
      "Requirement already satisfied: python-slugify in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from kaggle) (8.0.4)\n",
      "Requirement already satisfied: requests in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from kaggle) (2.32.5)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in c:\\program files\\python311\\lib\\site-packages (from kaggle) (65.5.0)\n",
      "Requirement already satisfied: six>=1.10 in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from kaggle) (1.17.0)\n",
      "Requirement already satisfied: text-unidecode in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from kaggle) (1.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from kaggle) (4.67.1)\n",
      "Requirement already satisfied: urllib3>=1.15.1 in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from kaggle) (2.5.0)\n",
      "Requirement already satisfied: webencodings in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from kaggle) (0.5.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\seanh\\appdata\\roaming\\python\\python311\\site-packages (from tqdm->kaggle) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting textstat\n",
      "  Downloading textstat-0.7.8-py3-none-any.whl (239 kB)\n",
      "     ---------------------------------------- 0.0/239.1 kB ? eta -:--:--\n",
      "     -------------------------------------- 239.1/239.1 kB 7.4 MB/s eta 0:00:00\n",
      "Collecting pyphen\n",
      "  Downloading pyphen-0.17.2-py3-none-any.whl (2.1 MB)\n",
      "     ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "     --------- ------------------------------ 0.5/2.1 MB 10.2 MB/s eta 0:00:01\n",
      "     ------------------- -------------------- 1.0/2.1 MB 10.9 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 1.6/2.1 MB 11.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.1/2.1 MB 12.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 2.1/2.1 MB 11.1 MB/s eta 0:00:00\n",
      "Collecting cmudict\n",
      "  Downloading cmudict-1.1.1-py3-none-any.whl (939 kB)\n",
      "     ---------------------------------------- 0.0/939.7 kB ? eta -:--:--\n",
      "     ---------------------- -------------- 563.2/939.7 kB 17.8 MB/s eta 0:00:01\n",
      "     ------------------------------------- 939.7/939.7 kB 11.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: setuptools in c:\\program files\\python311\\lib\\site-packages (from textstat) (65.5.0)\n",
      "Collecting importlib-metadata>=5\n",
      "  Downloading importlib_metadata-8.7.0-py3-none-any.whl (27 kB)\n",
      "Collecting importlib-resources>=5\n",
      "  Downloading importlib_resources-6.5.2-py3-none-any.whl (37 kB)\n",
      "Collecting zipp>=3.20\n",
      "  Downloading zipp-3.23.0-py3-none-any.whl (10 kB)\n",
      "Installing collected packages: zipp, pyphen, importlib-resources, importlib-metadata, cmudict, textstat\n",
      "Successfully installed cmudict-1.1.1 importlib-metadata-8.7.0 importlib-resources-6.5.2 pyphen-0.17.2 textstat-0.7.8 zipp-3.23.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "UsageError: Line magic function `%python` not found (But cell magic `%%python` exists, did you mean that instead?).\n"
     ]
    }
   ],
   "source": [
    "# ⚠️ Run this cell only if fresh runtime or first time setup\n",
    "\n",
    "# Install required packages\n",
    "%pip install transformers torch datasets pandas numpy scikit-learn matplotlib seaborn plotly\n",
    "%pip install huggingface-hub accelerate\n",
    "%pip install nltk spacy wordcloud\n",
    "%pip install kaggle\n",
    "%pip install textstat\n",
    "%python -m spacy download en_core_web_sm\n",
    "print(\"All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b987589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports successful!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\seanh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\seanh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\seanh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# ⚠️ Run this cell only if fresh runtime or first time setup\n",
    "\n",
    "# Import essential libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# NLP and ML libraries\n",
    "import nltk\n",
    "import spacy\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Data processing\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Terminal commands\n",
    "import os\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0bfc02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ⚠️ Run this cell only if fresh runtime or first time setup\n",
    "\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "# Kaggle API Setup & Downloading of Dataset to ./kaggle_data directory\n",
    "def config_kaggle_api_token():\n",
    "    # kaggle_dir = Path.home() / '.config' / 'kaggle'\n",
    "    kaggle_dir = Path.home() / '.kaggle'\n",
    "    kaggle_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    shutil.copy('./kaggle.json', kaggle_dir / 'kaggle.json')\n",
    "    os.chmod(kaggle_dir / 'kaggle.json', 0o600)\n",
    "\n",
    "def download_kaggle_dataset(path='./kaggle_data', dataset_name=\"denizbilginn/google-maps-restaurant-reviews\"):\n",
    "    api = KaggleApi()\n",
    "    api.authenticate()\n",
    "    dataset_name=\"denizbilginn/google-maps-restaurant-reviews\"\n",
    "    api.dataset_download_files(dataset_name,\n",
    "                            path=path,\n",
    "                            unzip=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7b39ba",
   "metadata": {},
   "source": [
    "## 📊 Data Collection & Loading\n",
    "\n",
    "We'll use the provided Google Local Reviews dataset. You can also supplement with additional data sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0c52343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ⚠️ Run this cell only if fresh runtime or first time setup\n",
    "\n",
    "# Download Kaggle Dataset\n",
    "# config_kaggle_api_token()\n",
    "# download_kaggle_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b012e71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loading Functions\n",
    "\n",
    "def load_dataset(file_path):\n",
    "    \"\"\"Load dataset from local CSV file\"\"\"\n",
    "    try:\n",
    "        if os.path.exists(file_path):\n",
    "            df = pd.read_csv(file_path)\n",
    "            print(f\"✅ Loaded {len(df)} rows from {file_path}\")\n",
    "            # df = standardize_columns(df)\n",
    "            return df\n",
    "        else:\n",
    "            print(f\"❌ File not found: {file_path}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading local file: {e}\")\n",
    "        return None\n",
    "\n",
    "def standardize_columns(df):\n",
    "    \"\"\"Standardize column names to match our expected format\"\"\"\n",
    "    # Common column mappings\n",
    "    column_mappings = {\n",
    "        'text': 'review_text',\n",
    "        'review': 'review_text',\n",
    "        'comment': 'review_text',\n",
    "        'content': 'review_text',\n",
    "        'review_text': 'review_text',\n",
    "\n",
    "        'rating': 'rating',\n",
    "        'stars': 'rating',\n",
    "        'score': 'rating',\n",
    "        'star_rating': 'rating',\n",
    "\n",
    "        'business': 'business_name',\n",
    "        'restaurant': 'business_name',\n",
    "        'place_name': 'business_name',\n",
    "        'name': 'business_name',\n",
    "\n",
    "        'user': 'user_id',\n",
    "        'user_name': 'user_id',\n",
    "        'reviewer': 'user_id',\n",
    "\n",
    "        'date': 'timestamp',\n",
    "        'time': 'timestamp',\n",
    "        'created_at': 'timestamp',\n",
    "        'review_date': 'timestamp'\n",
    "    }\n",
    "\n",
    "    # Convert column names to lowercase for matching\n",
    "    df_columns_lower = [col.lower() for col in df.columns]\n",
    "\n",
    "    # Apply mappings\n",
    "    new_columns = []\n",
    "    for col in df.columns:\n",
    "        col_lower = col.lower()\n",
    "        if col_lower in column_mappings:\n",
    "            new_columns.append(column_mappings[col_lower])\n",
    "        else:\n",
    "            new_columns.append(col)\n",
    "\n",
    "    df.columns = new_columns\n",
    "\n",
    "    # Ensure we have required columns\n",
    "    required_columns = ['review_text', 'rating']\n",
    "    for col in required_columns:\n",
    "        if col not in df.columns:\n",
    "            if col == 'review_text':\n",
    "                # Try to find any text column\n",
    "                text_cols = [c for c in df.columns if 'text' in c.lower() or 'review' in c.lower() or 'comment' in c.lower()]\n",
    "                if text_cols:\n",
    "                    df['review_text'] = df[text_cols[0]]\n",
    "                else:\n",
    "                    print(f\"⚠️ Could not find text column, creating placeholder\")\n",
    "                    df['review_text'] = \"Sample review text\"\n",
    "            elif col == 'rating':\n",
    "                # Try to find any rating column\n",
    "                rating_cols = [c for c in df.columns if 'rating' in c.lower() or 'star' in c.lower() or 'score' in c.lower()]\n",
    "                if rating_cols:\n",
    "                    df['rating'] = df[rating_cols[0]]\n",
    "                else:\n",
    "                    print(f\"⚠️ Could not find rating column, creating placeholder\")\n",
    "                    df['rating'] = 3  # Default neutral rating\n",
    "\n",
    "    # Add missing optional columns\n",
    "    if 'business_name' not in df.columns:\n",
    "        df['business_name'] = 'Unknown Business'\n",
    "    if 'user_id' not in df.columns:\n",
    "        df['user_id'] = [f'user_{i}' for i in range(len(df))]\n",
    "    if 'timestamp' not in df.columns:\n",
    "        df['timestamp'] = pd.date_range('2024-01-01', periods=len(df), freq='D')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c24329b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleanup\n",
    "\n",
    "def _find_col(df, aliases, required=True):\n",
    "    \"\"\"Return the first matching column from aliases; None if not found and required=False.\"\"\"\n",
    "    cols_lower = {c.lower(): c for c in df.columns}\n",
    "    for a in aliases:\n",
    "        if a.lower() in cols_lower:\n",
    "            return cols_lower[a.lower()]\n",
    "    if required:\n",
    "        raise KeyError(f\"None of the aliases {aliases} found in columns: {list(df.columns)}\")\n",
    "    return None\n",
    "\n",
    "def clean_reviews_dataset(df):\n",
    "    \"\"\"\n",
    "    Keep rows that have ALL of the following (non-empty, non-NaN):\n",
    "      - business_name\n",
    "      - author_name\n",
    "      - text\n",
    "      - rating\n",
    "    Allow missing: photo, rating_category\n",
    "    Preserve output columns in original schema.\n",
    "    \"\"\"\n",
    "\n",
    "    # Resolve columns even if earlier steps renamed them\n",
    "    col_business = _find_col(df, [\"business_name\", \"restaurant\", \"place_name\", \"name\"])\n",
    "    col_author   = _find_col(df, [\"author_name\", \"user\", \"user_name\", \"reviewer\"])\n",
    "    col_text     = _find_col(df, [\"text\", \"review_text\", \"comment\", \"content\"])\n",
    "    col_rating   = _find_col(df, [\"rating\", \"stars\", \"score\", \"star_rating\"])\n",
    "\n",
    "    # Optional columns may or may not exist\n",
    "    col_photo          = _find_col(df, [\"photo\"], required=False)\n",
    "    col_rating_category= _find_col(df, [\"rating_category\"], required=False)\n",
    "\n",
    "    # Work on a copy\n",
    "    d = df.copy()\n",
    "\n",
    "    # Normalize whitespace for string fields (only if they exist)\n",
    "    for c in [col_business, col_author, col_text]:\n",
    "        d[c] = d[c].astype(str).str.strip()\n",
    "\n",
    "    # Coerce rating to numeric\n",
    "    d[col_rating] = pd.to_numeric(d[col_rating], errors=\"coerce\")\n",
    "\n",
    "    # Drop rows with missing/empty required fields\n",
    "    before = len(d)\n",
    "    d = d.dropna(subset=[col_business, col_author, col_text, col_rating])\n",
    "    # Remove empty-string rows in required text columns\n",
    "    for c in [col_business, col_author, col_text]:\n",
    "        d = d[d[c] != \"\"]\n",
    "    # Optionally enforce valid rating range (comment out if you want raw)\n",
    "    d = d[(d[col_rating] >= 1) & (d[col_rating] <= 5)]\n",
    "\n",
    "    removed = before - len(d)\n",
    "    print(f\"🧹 Cleaned dataset: {before} → {len(d)} rows (removed {removed})\")\n",
    "\n",
    "    # Rebuild output with your target column names in the same format\n",
    "    out = pd.DataFrame({\n",
    "        \"business_name\":    d[col_business],\n",
    "        \"author_name\":      d[col_author],\n",
    "        \"text\":             d[col_text],\n",
    "        \"rating\":           d[col_rating],\n",
    "    })\n",
    "\n",
    "    # Attach optional columns if present; else create with NaN\n",
    "    out[\"photo\"] = d[col_photo] if col_photo in d.columns else pd.Series([pd.NA]*len(d))\n",
    "    out[\"rating_category\"] = d[col_rating_category] if col_rating_category in d.columns else pd.Series([pd.NA]*len(d))\n",
    "\n",
    "    # Keep any extra columns? If you want to strictly keep only the six, return `out` as is.\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bd1608a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 1100 rows from ./kaggle_data/reviews.csv\n",
      "🔧 Introduced a missing value in row 5 (text column)\n",
      "\n",
      "🧹 Cleaned dataset: 1100 → 1100 rows (removed 0)\n",
      "\n",
      "📋 Cleaned Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1100 entries, 0 to 1099\n",
      "Data columns (total 6 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   business_name    1100 non-null   object\n",
      " 1   author_name      1100 non-null   object\n",
      " 2   text             1100 non-null   object\n",
      " 3   rating           1100 non-null   int64 \n",
      " 4   photo            1100 non-null   object\n",
      " 5   rating_category  1100 non-null   object\n",
      "dtypes: int64(1), object(5)\n",
      "memory usage: 51.7+ KB\n",
      "None\n",
      "\n",
      "📊 Dataset shape: (1100, 6)\n",
      "\n",
      "🔍 First 5 reviews:\n",
      "                     business_name    author_name  \\\n",
      "0  Haci'nin Yeri - Yigit Lokantasi    Gulsum Akar   \n",
      "1  Haci'nin Yeri - Yigit Lokantasi  Oguzhan Cetin   \n",
      "2  Haci'nin Yeri - Yigit Lokantasi     Yasin Kuyu   \n",
      "3  Haci'nin Yeri - Yigit Lokantasi     Orhan Kapu   \n",
      "4  Haci'nin Yeri - Yigit Lokantasi     Ozgur Sati   \n",
      "\n",
      "                                                text  rating  \\\n",
      "0  We went to Marmaris with my wife for a holiday...       5   \n",
      "1  During my holiday in Marmaris we ate here to f...       4   \n",
      "2  Prices are very affordable. The menu in the ph...       3   \n",
      "3  Turkey's cheapest artisan restaurant and its f...       5   \n",
      "4  I don't know what you will look for in terms o...       3   \n",
      "\n",
      "                                               photo     rating_category  \n",
      "0         dataset/taste/hacinin_yeri_gulsum_akar.png               taste  \n",
      "1        dataset/menu/hacinin_yeri_oguzhan_cetin.png                menu  \n",
      "2  dataset/outdoor_atmosphere/hacinin_yeri_yasin_...  outdoor_atmosphere  \n",
      "3  dataset/indoor_atmosphere/hacinin_yeri_orhan_k...   indoor_atmosphere  \n",
      "4           dataset/menu/hacinin_yeri_ozgur_sati.png                menu  \n",
      "\n",
      "✅ Data Quality Check:\n",
      "- Total reviews: 1100\n",
      "- Unique businesses: 100\n",
      "- Rating distribution: {1: np.int64(80), 2: np.int64(72), 3: np.int64(172), 4: np.int64(316), 5: np.int64(460)}\n",
      "- Missing values: 0\n",
      "- Average review length: 110.8 characters\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = load_dataset('./kaggle_data/reviews.csv')\n",
    "\n",
    "# 👇 Simulate a bad row (make the 5th row's text missing)\n",
    "df.loc[4, \"review_text\"] = \"\"   # or \"\" to test empty-string removal\n",
    "print(\"🔧 Introduced a missing value in row 5 (text column)\\n\")\n",
    "\n",
    "df = clean_reviews_dataset(df)\n",
    "\n",
    "print(\"\\n📋 Cleaned Dataset Info:\")\n",
    "print(df.info())\n",
    "print(f\"\\n📊 Dataset shape: {df.shape}\")\n",
    "print(\"\\n🔍 First 5 reviews:\")\n",
    "print(df.head())\n",
    "\n",
    "# Display data quality info\n",
    "print(f\"\\n✅ Data Quality Check:\")\n",
    "print(f\"- Total reviews: {len(df)}\")\n",
    "print(f\"- Unique businesses: {df['business_name'].nunique()}\")\n",
    "print(f\"- Rating distribution: {dict(df['rating'].value_counts().sort_index())}\")\n",
    "print(f\"- Missing values: {df.isnull().sum().sum()}\")\n",
    "print(f\"- Average review length: {df['text'].str.len().mean():.1f} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9692e1",
   "metadata": {},
   "source": [
    "## 🔧 Feature Engineering\n",
    "\n",
    "Extract comprehensive textual and non-textual features from review data for ML models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "faad4094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ textstat module loaded successfully\n",
      "🔧 Extracting features...\n",
      "📋 Available columns: ['business_name', 'author_name', 'text', 'rating', 'photo', 'rating_category']\n",
      "✅ Using 'text' as the text column\n",
      "   Processing row 0/1100\n",
      "   Processing row 1000/1100\n",
      "✅ Feature extraction complete! Extracted 44 features\n",
      "\n",
      "📊 Dataset with features shape: (1100, 50)\n",
      "\n",
      "🔍 Feature columns extracted:\n",
      "    1. text_length\n",
      "    2. word_count\n",
      "    3. sentence_count\n",
      "    4. avg_word_length\n",
      "    5. uppercase_count\n",
      "    6. punctuation_count\n",
      "    7. digit_count\n",
      "    8. exclamation_count\n",
      "    9. question_count\n",
      "   10. uppercase_ratio\n",
      "   11. punctuation_ratio\n",
      "   12. digit_ratio\n",
      "   13. sentiment_pos\n",
      "   14. sentiment_neu\n",
      "   15. sentiment_neg\n",
      "   16. sentiment_compound\n",
      "   17. has_url\n",
      "   18. has_email\n",
      "   19. has_phone\n",
      "   20. first_person_count\n",
      "   21. second_person_count\n",
      "   22. third_person_count\n",
      "   23. first_person_ratio\n",
      "   24. second_person_ratio\n",
      "   25. third_person_ratio\n",
      "   26. spam_keyword_count\n",
      "   27. spam_keyword_ratio\n",
      "   28. restaurant_keyword_count\n",
      "   29. restaurant_keyword_ratio\n",
      "   30. flesch_reading_ease\n",
      "   31. flesch_kincaid_grade\n",
      "   32. all_caps_word_count\n",
      "   33. all_caps_ratio\n",
      "   34. unique_word_ratio\n",
      "   35. rating\n",
      "   36. is_extreme_rating\n",
      "   37. is_low_rating\n",
      "   38. is_high_rating\n",
      "   39. is_neutral_rating\n",
      "   40. author_name_length\n",
      "   41. author_has_numbers\n",
      "   42. author_all_caps\n",
      "   43. business_name_length\n",
      "   44. has_photo\n",
      "\n",
      "📈 Feature Statistics Summary:\n",
      "       text_length  word_count  sentence_count  avg_word_length  \\\n",
      "count     1100.000    1100.000        1100.000         1100.000   \n",
      "mean       110.834      20.052           2.145            4.807   \n",
      "std         69.154      12.978           1.233            0.947   \n",
      "min          5.000       1.000           1.000            3.269   \n",
      "25%         62.000      11.000           1.000            4.248   \n",
      "50%        104.000      19.000           2.000            4.615   \n",
      "75%        147.000      27.000           3.000            5.056   \n",
      "max        914.000     179.000          17.000           12.000   \n",
      "\n",
      "       uppercase_count  punctuation_count  digit_count  exclamation_count  \\\n",
      "count         1100.000           1100.000     1100.000           1100.000   \n",
      "mean             2.768              3.663        0.313              0.085   \n",
      "std              1.999              2.213        1.090              0.321   \n",
      "min              0.000              0.000        0.000              0.000   \n",
      "25%              1.000              2.000        0.000              0.000   \n",
      "50%              2.000              3.000        0.000              0.000   \n",
      "75%              3.000              5.000        0.000              0.000   \n",
      "max             23.000             22.000        9.000              4.000   \n",
      "\n",
      "       question_count  uppercase_ratio  ...    rating  is_extreme_rating  \\\n",
      "count        1100.000         1100.000  ...  1100.000           1100.000   \n",
      "mean            0.006            0.030  ...     3.913              0.491   \n",
      "std             0.080            0.024  ...     1.218              0.500   \n",
      "min             0.000            0.000  ...     1.000              0.000   \n",
      "25%             0.000            0.018  ...     3.000              0.000   \n",
      "50%             0.000            0.025  ...     4.000              0.000   \n",
      "75%             0.000            0.035  ...     5.000              1.000   \n",
      "max             1.000            0.333  ...     5.000              1.000   \n",
      "\n",
      "       is_low_rating  is_high_rating  is_neutral_rating  author_name_length  \\\n",
      "count       1100.000        1100.000           1100.000            1100.000   \n",
      "mean           0.138           0.705              0.156              12.005   \n",
      "std            0.345           0.456              0.363               2.069   \n",
      "min            0.000           0.000              0.000               6.000   \n",
      "25%            0.000           0.000              0.000              11.000   \n",
      "50%            0.000           1.000              0.000              12.000   \n",
      "75%            0.000           1.000              0.000              13.000   \n",
      "max            1.000           1.000              1.000              20.000   \n",
      "\n",
      "       author_has_numbers  author_all_caps  business_name_length  has_photo  \n",
      "count              1100.0           1100.0              1100.000     1100.0  \n",
      "mean                  0.0              0.0                12.300        1.0  \n",
      "std                   0.0              0.0                 5.085        0.0  \n",
      "min                   0.0              0.0                 3.000        1.0  \n",
      "25%                   0.0              0.0                 8.000        1.0  \n",
      "50%                   0.0              0.0                12.000        1.0  \n",
      "75%                   0.0              0.0                16.000        1.0  \n",
      "max                   0.0              0.0                31.000        1.0  \n",
      "\n",
      "[8 rows x 41 columns]\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from datetime import datetime\n",
    "\n",
    "# Try to import textstat, use fallback if not available\n",
    "try:\n",
    "    from textstat import flesch_reading_ease, flesch_kincaid_grade\n",
    "    TEXTSTAT_AVAILABLE = True\n",
    "    print(\"✅ textstat module loaded successfully\")\n",
    "except ImportError:\n",
    "    print(\"⚠️ textstat module not found. Readability metrics will be set to default values.\")\n",
    "    print(\"💡 To install: pip install textstat\")\n",
    "    TEXTSTAT_AVAILABLE = False\n",
    "    \n",
    "    # Fallback functions\n",
    "    def flesch_reading_ease(text):\n",
    "        return 50.0  # Default neutral readability score\n",
    "    \n",
    "    def flesch_kincaid_grade(text):\n",
    "        return 8.0   # Default grade level\n",
    "\n",
    "class AdvancedFeatureExtractor:\n",
    "    \"\"\"Extract comprehensive textual and non-textual features from review data\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.sia = SentimentIntensityAnalyzer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        \n",
    "        # Common spam/promotional keywords\n",
    "        self.spam_keywords = [\n",
    "            'discount', 'promo', 'deal', 'offer', 'sale', 'buy', 'purchase', \n",
    "            'visit', 'click', 'link', 'website', 'free', 'win', 'prize'\n",
    "        ]\n",
    "        \n",
    "        # Restaurant-related keywords for relevancy\n",
    "        self.restaurant_keywords = [\n",
    "            'food', 'meal', 'eat', 'taste', 'flavor', 'delicious', 'menu',\n",
    "            'service', 'waiter', 'waitress', 'staff', 'cook', 'chef',\n",
    "            'restaurant', 'cafe', 'dine', 'dining', 'lunch', 'dinner',\n",
    "            'breakfast', 'appetizer', 'entree', 'dessert', 'drink'\n",
    "        ]\n",
    "        \n",
    "    def extract_textual_features(self, text):\n",
    "        \"\"\"Extract comprehensive textual features\"\"\"\n",
    "        features = {}\n",
    "        text_lower = text.lower()\n",
    "        words = text.split()\n",
    "        \n",
    "        # Basic text statistics\n",
    "        features['text_length'] = len(text)\n",
    "        features['word_count'] = len(words)\n",
    "        features['sentence_count'] = len([s for s in text.split('.') if s.strip()])\n",
    "        features['avg_word_length'] = np.mean([len(word) for word in words]) if words else 0\n",
    "        \n",
    "        # Character-level features\n",
    "        features['uppercase_count'] = sum(1 for c in text if c.isupper())\n",
    "        features['punctuation_count'] = sum(1 for c in text if c in string.punctuation)\n",
    "        features['digit_count'] = sum(1 for c in text if c.isdigit())\n",
    "        features['exclamation_count'] = text.count('!')\n",
    "        features['question_count'] = text.count('?')\n",
    "        \n",
    "        # Ratios\n",
    "        total_chars = len(text) if len(text) > 0 else 1\n",
    "        features['uppercase_ratio'] = features['uppercase_count'] / total_chars\n",
    "        features['punctuation_ratio'] = features['punctuation_count'] / total_chars\n",
    "        features['digit_ratio'] = features['digit_count'] / total_chars\n",
    "        \n",
    "        # Sentiment analysis\n",
    "        sentiment_scores = self.sia.polarity_scores(text)\n",
    "        features.update({\n",
    "            'sentiment_pos': sentiment_scores['pos'],\n",
    "            'sentiment_neu': sentiment_scores['neu'],\n",
    "            'sentiment_neg': sentiment_scores['neg'],\n",
    "            'sentiment_compound': sentiment_scores['compound']\n",
    "        })\n",
    "        \n",
    "        # URL and contact detection\n",
    "        features['has_url'] = bool(re.search(r'http[s]?://\\S+|www\\.\\w+\\.\\w+', text_lower))\n",
    "        features['has_email'] = bool(re.search(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', text))\n",
    "        features['has_phone'] = bool(re.search(r'\\b\\d{3}[-.]?\\d{3}[-.]?\\d{4}\\b', text))\n",
    "        \n",
    "        # Personal pronouns and perspective\n",
    "        first_person_words = ['i', 'me', 'my', 'myself', 'we', 'us', 'our']\n",
    "        second_person_words = ['you', 'your', 'yours']\n",
    "        third_person_words = ['he', 'she', 'it', 'they', 'them', 'their']\n",
    "        \n",
    "        words_lower = [w.lower().strip(string.punctuation) for w in words]\n",
    "        features['first_person_count'] = sum(1 for word in words_lower if word in first_person_words)\n",
    "        features['second_person_count'] = sum(1 for word in words_lower if word in second_person_words)\n",
    "        features['third_person_count'] = sum(1 for word in words_lower if word in third_person_words)\n",
    "        \n",
    "        total_words = len(words) if len(words) > 0 else 1\n",
    "        features['first_person_ratio'] = features['first_person_count'] / total_words\n",
    "        features['second_person_ratio'] = features['second_person_count'] / total_words\n",
    "        features['third_person_ratio'] = features['third_person_count'] / total_words\n",
    "        \n",
    "        # Spam/promotional indicators\n",
    "        features['spam_keyword_count'] = sum(1 for keyword in self.spam_keywords if keyword in text_lower)\n",
    "        features['spam_keyword_ratio'] = features['spam_keyword_count'] / total_words\n",
    "        \n",
    "        # Restaurant relevancy indicators\n",
    "        features['restaurant_keyword_count'] = sum(1 for keyword in self.restaurant_keywords if keyword in text_lower)\n",
    "        features['restaurant_keyword_ratio'] = features['restaurant_keyword_count'] / total_words\n",
    "        \n",
    "        # Readability metrics with fallback\n",
    "        try:\n",
    "            if TEXTSTAT_AVAILABLE:\n",
    "                features['flesch_reading_ease'] = flesch_reading_ease(text)\n",
    "                features['flesch_kincaid_grade'] = flesch_kincaid_grade(text)\n",
    "            else:\n",
    "                # Use simple fallback calculations\n",
    "                avg_sentence_length = features['word_count'] / max(features['sentence_count'], 1)\n",
    "                features['flesch_reading_ease'] = max(0, min(100, 206.835 - (1.015 * avg_sentence_length) - (84.6 * features['avg_word_length'])))\n",
    "                features['flesch_kincaid_grade'] = max(0, (0.39 * avg_sentence_length) + (11.8 * features['avg_word_length']) - 15.59)\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Readability calculation failed: {e}\")\n",
    "            features['flesch_reading_ease'] = 50.0  # Default neutral score\n",
    "            features['flesch_kincaid_grade'] = 8.0   # Default grade level\n",
    "        \n",
    "        # All caps words (often indicates spam/shouting)\n",
    "        all_caps_words = [w for w in words if w.isupper() and len(w) > 1]\n",
    "        features['all_caps_word_count'] = len(all_caps_words)\n",
    "        features['all_caps_ratio'] = len(all_caps_words) / total_words\n",
    "        \n",
    "        # Repetitive patterns\n",
    "        unique_words = set(words_lower)\n",
    "        features['unique_word_ratio'] = len(unique_words) / total_words\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def extract_non_textual_features(self, row):\n",
    "        \"\"\"Extract non-textual features from metadata\"\"\"\n",
    "        features = {}\n",
    "        \n",
    "        # Rating-based features\n",
    "        features['rating'] = row['rating']\n",
    "        features['is_extreme_rating'] = 1 if row['rating'] in [1, 5] else 0\n",
    "        features['is_low_rating'] = 1 if row['rating'] <= 2 else 0\n",
    "        features['is_high_rating'] = 1 if row['rating'] >= 4 else 0\n",
    "        features['is_neutral_rating'] = 1 if row['rating'] == 3 else 0\n",
    "        \n",
    "        # Author-based features\n",
    "        if 'author_name' in row:\n",
    "            author_name = str(row['author_name'])\n",
    "            features['author_name_length'] = len(author_name)\n",
    "            features['author_has_numbers'] = 1 if any(c.isdigit() for c in author_name) else 0\n",
    "            features['author_all_caps'] = 1 if author_name.isupper() else 0\n",
    "        else:\n",
    "            features['author_name_length'] = 0\n",
    "            features['author_has_numbers'] = 0\n",
    "            features['author_all_caps'] = 0\n",
    "        \n",
    "        # Business-based features\n",
    "        if 'business_name' in row:\n",
    "            business_name = str(row['business_name'])\n",
    "            features['business_name_length'] = len(business_name)\n",
    "        else:\n",
    "            features['business_name_length'] = 0\n",
    "        \n",
    "        # Photo presence\n",
    "        if 'photo' in row and pd.notna(row['photo']):\n",
    "            features['has_photo'] = 1\n",
    "        else:\n",
    "            features['has_photo'] = 0\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def extract_all_features(self, df):\n",
    "        \"\"\"Extract all features for the entire dataset\"\"\"\n",
    "        print(\"🔧 Extracting features...\")\n",
    "        if not TEXTSTAT_AVAILABLE:\n",
    "            print(\"⚠️ Using fallback readability calculations (textstat not available)\")\n",
    "        \n",
    "        # 🔍 COLUMN NAME DETECTION AND VALIDATION\n",
    "        print(f\"📋 Available columns: {list(df.columns)}\")\n",
    "        \n",
    "        # Find the text column dynamically\n",
    "        text_column = None\n",
    "        possible_text_columns = ['text', 'review_text', 'review', 'content', 'comment']\n",
    "        \n",
    "        for col in possible_text_columns:\n",
    "            if col in df.columns:\n",
    "                text_column = col\n",
    "                break\n",
    "        \n",
    "        if text_column is None:\n",
    "            raise ValueError(f\"❌ No text column found! Available columns: {list(df.columns)}\")\n",
    "        \n",
    "        print(f\"✅ Using '{text_column}' as the text column\")\n",
    "        \n",
    "        all_features = []\n",
    "        \n",
    "        for idx, row in df.iterrows():\n",
    "            if idx % 1000 == 0:\n",
    "                print(f\"   Processing row {idx}/{len(df)}\")\n",
    "            \n",
    "            # Use the detected text column instead of hardcoded 'review_text'\n",
    "            text_features = self.extract_textual_features(row[text_column])\n",
    "            non_text_features = self.extract_non_textual_features(row)\n",
    "            \n",
    "            # Combine all features\n",
    "            combined_features = {**text_features, **non_text_features}\n",
    "            all_features.append(combined_features)\n",
    "        \n",
    "        features_df = pd.DataFrame(all_features)\n",
    "        print(f\"✅ Feature extraction complete! Extracted {len(features_df.columns)} features\")\n",
    "        return features_df\n",
    "\n",
    "# Initialize feature extractor\n",
    "feature_extractor = AdvancedFeatureExtractor()\n",
    "\n",
    "# Extract features from the dataset\n",
    "features_df = feature_extractor.extract_all_features(df)\n",
    "\n",
    "# Combine with original data\n",
    "df_with_features = pd.concat([df.reset_index(drop=True), features_df], axis=1)\n",
    "\n",
    "print(f\"\\n📊 Dataset with features shape: {df_with_features.shape}\")\n",
    "print(f\"\\n🔍 Feature columns extracted:\")\n",
    "for i, col in enumerate(features_df.columns, 1):\n",
    "    print(f\"   {i:2d}. {col}\")\n",
    "\n",
    "# Display feature statistics\n",
    "print(\"\\n📈 Feature Statistics Summary:\")\n",
    "print(features_df.describe().round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ff5045",
   "metadata": {},
   "source": [
    "## 🚫 Policy Detection Module\n",
    "\n",
    "Implement rule-based and ML-based policy violation detectors for the three main categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "195e4d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Policy Violation Detection Results:\n",
      "==================================================\n",
      "\n",
      "📝 Review 1: 'Great food and excellent service! The pasta was am...'\n",
      "🚨 Violation: False\n",
      "💡 Details: No violation detected\n",
      "\n",
      "📝 Review 2: 'Call us now for the best deals! Visit our website ...'\n",
      "🚨 Violation: True\n",
      "📋 Type: advertisement\n",
      "🎯 Confidence: 0.333\n",
      "📊 All Scores: {'advertisement': 0.3333333333333333, 'irrelevant_content': 0, 'rant_without_visit': 0.0}\n",
      "💡 Details: Primary violation: advertisement\n",
      "\n",
      "📝 Review 3: 'I hate politics and this election is terrible. Not...'\n",
      "🚨 Violation: False\n",
      "💡 Details: No violation detected\n",
      "\n",
      "📝 Review 4: 'I heard this place is awful, never been there but ...'\n",
      "🚨 Violation: True\n",
      "📋 Type: rant_without_visit\n",
      "🎯 Confidence: 0.357\n",
      "📊 All Scores: {'advertisement': 0.0, 'irrelevant_content': 0, 'rant_without_visit': 0.35714285714285715}\n",
      "💡 Details: Primary violation: rant_without_visit\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from typing import Dict, Tuple, List\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class PolicyViolationDetector:\n",
    "    \"\"\"Rule-based policy violation detector for restaurant reviews\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Advertisement detection patterns\n",
    "        self.ad_patterns = [\n",
    "            r'\\b(?:call|text|contact|visit|website|phone|email|dm|message)\\s+(?:us|me|now|today)\\b',\n",
    "            r'\\b(?:best|cheapest|lowest|highest|top)\\s+(?:price|deal|offer|service)\\b',\n",
    "            r'\\b(?:free|discount|sale|promo|special|offer|deal)\\b.*\\b(?:today|now|limited|expires)\\b',\n",
    "            r'\\b(?:check|visit|see|follow)\\s+(?:our|my)\\s+(?:website|page|profile|instagram|facebook)\\b',\n",
    "            r'\\b(?:book|order|reserve)\\s+(?:now|today|online)\\b',\n",
    "            r'(?:www\\.|http|\\.com|\\.org|\\.net)',\n",
    "            r'\\b(?:delivery|takeout|pickup)\\s+(?:available|service)\\b',\n",
    "            r'\\b(?:new|grand)\\s+opening\\b',\n",
    "            r'\\b(?:hiring|recruiting|looking\\s+for)\\b'\n",
    "        ]\n",
    "        \n",
    "        # Irrelevant content patterns\n",
    "        self.irrelevant_patterns = [\n",
    "            r'\\b(?:politics|election|government|president|mayor|council)\\b',\n",
    "            r'\\b(?:religion|church|mosque|temple|spiritual)\\b',\n",
    "            r'\\b(?:personal|relationship|dating|marriage|divorce)\\b',\n",
    "            r'\\b(?:medical|health|doctor|hospital|surgery|medicine)\\b',\n",
    "            r'\\b(?:school|education|homework|exam|grade)\\b',\n",
    "            r'\\b(?:weather|rain|snow|sunny|cloudy)\\b',\n",
    "            r'\\b(?:sports|game|match|team|player|score)\\b',\n",
    "            r'\\b(?:movie|film|tv|show|actor|actress)\\b',\n",
    "            r'\\b(?:music|song|concert|band|album)\\b',\n",
    "            r'\\b(?:car|vehicle|traffic|parking|driving)\\b'\n",
    "        ]\n",
    "        \n",
    "        # Rant without visit patterns\n",
    "        self.rant_patterns = [\n",
    "            r'\\b(?:never\\s+(?:been|visited|went)|haven\\'t\\s+(?:been|visited))\\b',\n",
    "            r'\\b(?:heard|read|saw)\\s+(?:about|reviews|complaints)\\b',\n",
    "            r'\\b(?:based\\s+on|according\\s+to)\\s+(?:reviews|others|friends)\\b',\n",
    "            r'\\b(?:planning\\s+to|might|considering)\\s+(?:visit|go|try)\\b',\n",
    "            r'\\b(?:looks|seems|appears)\\s+(?:bad|terrible|awful|horrible)\\b',\n",
    "            r'\\b(?:reputation|known\\s+for)\\s+(?:being|having)\\b',\n",
    "            r'\\b(?:everyone\\s+says|people\\s+say|i\\'ve\\s+heard)\\b'\n",
    "        ]\n",
    "        \n",
    "        # Restaurant-related keywords (for relevance check)\n",
    "        self.restaurant_keywords = [\n",
    "            'food', 'meal', 'dish', 'restaurant', 'cafe', 'bar', 'service', 'waiter', 'waitress',\n",
    "            'menu', 'order', 'taste', 'flavor', 'delicious', 'cook', 'chef', 'kitchen',\n",
    "            'eat', 'dine', 'dining', 'lunch', 'dinner', 'breakfast', 'appetizer', 'dessert',\n",
    "            'drink', 'beverage', 'wine', 'beer', 'cocktail', 'table', 'reservation', 'staff'\n",
    "        ]\n",
    "    \n",
    "    def detect_advertisement(self, text: str) -> Tuple[bool, float, List[str]]:\n",
    "        \"\"\"Detect advertisement content\"\"\"\n",
    "        text_lower = text.lower()\n",
    "        matches = []\n",
    "        score = 0\n",
    "        \n",
    "        for pattern in self.ad_patterns:\n",
    "            if re.search(pattern, text_lower):\n",
    "                matches.append(pattern)\n",
    "                score += 1\n",
    "        \n",
    "        # Normalize score\n",
    "        confidence = min(score / len(self.ad_patterns), 1.0)\n",
    "        is_ad = confidence > 0.3\n",
    "        \n",
    "        return is_ad, confidence, matches\n",
    "    \n",
    "    def detect_irrelevant_content(self, text: str) -> Tuple[bool, float, List[str]]:\n",
    "        \"\"\"Detect irrelevant content\"\"\"\n",
    "        text_lower = text.lower()\n",
    "        matches = []\n",
    "        irrelevant_score = 0\n",
    "        relevant_score = 0\n",
    "        \n",
    "        # Check for irrelevant patterns\n",
    "        for pattern in self.irrelevant_patterns:\n",
    "            if re.search(pattern, text_lower):\n",
    "                matches.append(pattern)\n",
    "                irrelevant_score += 1\n",
    "        \n",
    "        # Check for restaurant relevance\n",
    "        for keyword in self.restaurant_keywords:\n",
    "            if keyword in text_lower:\n",
    "                relevant_score += 1\n",
    "        \n",
    "        # Calculate confidence\n",
    "        total_patterns = len(self.irrelevant_patterns)\n",
    "        if irrelevant_score > 0 and relevant_score == 0:\n",
    "            confidence = min(irrelevant_score / total_patterns, 1.0)\n",
    "            is_irrelevant = confidence > 0.2\n",
    "        else:\n",
    "            confidence = max(0, (irrelevant_score - relevant_score * 0.5) / total_patterns)\n",
    "            is_irrelevant = confidence > 0.3\n",
    "        \n",
    "        return is_irrelevant, max(confidence, 0), matches\n",
    "    \n",
    "    def detect_rant_without_visit(self, text: str) -> Tuple[bool, float, List[str]]:\n",
    "        \"\"\"Detect rants without actual visit\"\"\"\n",
    "        text_lower = text.lower()\n",
    "        matches = []\n",
    "        score = 0\n",
    "        \n",
    "        for pattern in self.rant_patterns:\n",
    "            if re.search(pattern, text_lower):\n",
    "                matches.append(pattern)\n",
    "                score += 1\n",
    "        \n",
    "        # Additional checks for negative sentiment without visit indicators\n",
    "        negative_words = ['terrible', 'awful', 'horrible', 'worst', 'disgusting', 'hate']\n",
    "        visit_indicators = ['went', 'visited', 'ate', 'ordered', 'tried', 'had dinner', 'had lunch']\n",
    "        \n",
    "        has_negative = any(word in text_lower for word in negative_words)\n",
    "        has_visit = any(indicator in text_lower for indicator in visit_indicators)\n",
    "        \n",
    "        if has_negative and not has_visit:\n",
    "            score += 0.5\n",
    "        \n",
    "        # Normalize score\n",
    "        confidence = min(score / len(self.rant_patterns), 1.0)\n",
    "        is_rant = confidence > 0.3\n",
    "        \n",
    "        return is_rant, confidence, matches\n",
    "    \n",
    "    def analyze_review(self, text: str) -> Dict:\n",
    "        \"\"\"Comprehensive policy violation analysis\"\"\"\n",
    "        if not text or len(text.strip()) < 10:\n",
    "            return {\n",
    "                'is_violation': False,\n",
    "                'violation_type': None,\n",
    "                'confidence': 0.0,\n",
    "                'details': 'Text too short for analysis'\n",
    "            }\n",
    "        \n",
    "        # Run all detectors\n",
    "        is_ad, ad_conf, ad_matches = self.detect_advertisement(text)\n",
    "        is_irrelevant, irr_conf, irr_matches = self.detect_irrelevant_content(text)\n",
    "        is_rant, rant_conf, rant_matches = self.detect_rant_without_visit(text)\n",
    "        \n",
    "        # Determine primary violation\n",
    "        violations = [\n",
    "            ('advertisement', ad_conf, ad_matches),\n",
    "            ('irrelevant_content', irr_conf, irr_matches),\n",
    "            ('rant_without_visit', rant_conf, rant_matches)\n",
    "        ]\n",
    "        \n",
    "        violations.sort(key=lambda x: x[1], reverse=True)\n",
    "        primary_violation = violations[0]\n",
    "        \n",
    "        is_violation = primary_violation[1] > 0.3\n",
    "        \n",
    "        return {\n",
    "            'is_violation': is_violation,\n",
    "            'violation_type': primary_violation[0] if is_violation else None,\n",
    "            'confidence': primary_violation[1],\n",
    "            'all_scores': {\n",
    "                'advertisement': ad_conf,\n",
    "                'irrelevant_content': irr_conf,\n",
    "                'rant_without_visit': rant_conf\n",
    "            },\n",
    "            'matches': primary_violation[2] if is_violation else [],\n",
    "            'details': f\"Primary violation: {primary_violation[0]}\" if is_violation else \"No violation detected\"\n",
    "        }\n",
    "\n",
    "# Initialize policy detector\n",
    "policy_detector = PolicyViolationDetector()\n",
    "\n",
    "# Test with sample reviews\n",
    "test_reviews = [\n",
    "    \"Great food and excellent service! The pasta was amazing.\",\n",
    "    \"Call us now for the best deals! Visit our website www.example.com\",\n",
    "    \"I hate politics and this election is terrible. Nothing about food here.\",\n",
    "    \"I heard this place is awful, never been there but people say it's bad.\"\n",
    "]\n",
    "\n",
    "print(\"🔍 Policy Violation Detection Results:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i, review in enumerate(test_reviews, 1):\n",
    "    result = policy_detector.analyze_review(review)\n",
    "    print(f\"\\n📝 Review {i}: '{review[:50]}...'\")\n",
    "    print(f\"🚨 Violation: {result['is_violation']}\")\n",
    "    if result['is_violation']:\n",
    "        print(f\"📋 Type: {result['violation_type']}\")\n",
    "        print(f\"🎯 Confidence: {result['confidence']:.3f}\")\n",
    "        print(f\"📊 All Scores: {result['all_scores']}\")\n",
    "    print(f\"💡 Details: {result['details']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758bcf29",
   "metadata": {},
   "source": [
    "## 🤖 Gemma 3 12B Model Integration\n",
    "\n",
    "Using Google's Gemma 3 12B model with HuggingFace Inference Client for advanced policy detection and review classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cdb91c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Initializing Gemma 3 12B Classifier...\n",
      "💡 Note: You may need a HuggingFace token for full functionality\n",
      "✅ Successfully initialized Gemma 3 12B model: google/gemma-3-12b-it\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "import json\n",
    "import time\n",
    "from typing import Dict, List, Optional\n",
    "import os\n",
    "\n",
    "class GemmaReviewClassifier:\n",
    "    \"\"\"Advanced review classifier using Gemma 3 12B model\"\"\"\n",
    "    \n",
    "    def __init__(self, hf_token: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        Initialize Gemma classifier\n",
    "        \n",
    "        Args:\n",
    "            hf_token: HuggingFace token (optional, can be set in environment)\n",
    "        \"\"\"\n",
    "        self.model_name = \"google/gemma-3-12b-it\"\n",
    "        \n",
    "        # Set up HuggingFace token\n",
    "        if hf_token:\n",
    "            os.environ[\"HUGGINGFACE_HUB_TOKEN\"] = hf_token\n",
    "        \n",
    "        try:\n",
    "            self.client = InferenceClient(\n",
    "                model=self.model_name,\n",
    "                token=hf_token or os.getenv(\"HUGGINGFACE_HUB_TOKEN\")\n",
    "            )\n",
    "            print(f\"✅ Successfully initialized Gemma 3 12B model: {self.model_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Warning: Could not initialize model. Error: {e}\")\n",
    "            print(\"💡 Note: You may need to provide a HuggingFace token or use a fallback model\")\n",
    "            self.client = None\n",
    "    \n",
    "    def create_policy_prompt(self, review_text: str) -> str:\n",
    "        \"\"\"Create a structured prompt for policy violation detection\"\"\"\n",
    "        \n",
    "        prompt = f\"\"\"You are an expert content moderator for restaurant review platforms. Analyze the following review and determine if it violates any of these policies:\n",
    "\n",
    "1. **Advertisement**: Reviews that promote businesses, include contact information, or solicit customers\n",
    "2. **Irrelevant Content**: Reviews about topics unrelated to the restaurant experience\n",
    "3. **Rant Without Visit**: Negative reviews from people who haven't actually visited the restaurant\n",
    "\n",
    "Review to analyze: \"{review_text}\"\n",
    "\n",
    "Please respond with a JSON object in this exact format:\n",
    "{{\n",
    "    \"is_violation\": true/false,\n",
    "    \"violation_type\": \"advertisement\" or \"irrelevant_content\" or \"rant_without_visit\" or null,\n",
    "    \"confidence\": 0.0-1.0,\n",
    "    \"reasoning\": \"Brief explanation of your decision\",\n",
    "    \"is_trustworthy\": true/false,\n",
    "    \"sentiment\": \"positive\" or \"negative\" or \"neutral\"\n",
    "}}\n",
    "\n",
    "Focus on:\n",
    "- Clear policy violations vs. legitimate reviews\n",
    "- Evidence of actual restaurant visit\n",
    "- Commercial intent vs. genuine feedback\n",
    "- Restaurant relevance vs. off-topic content\n",
    "\n",
    "Response:\"\"\"\n",
    "        \n",
    "        return prompt\n",
    "    \n",
    "    def create_quality_prompt(self, review_text: str) -> str:\n",
    "        \"\"\"Create a prompt for review quality assessment\"\"\"\n",
    "        \n",
    "        prompt = f\"\"\"As an expert in restaurant review quality assessment, evaluate this review for trustworthiness and usefulness:\n",
    "\n",
    "Review: \"{review_text}\"\n",
    "\n",
    "Assess the review on these dimensions:\n",
    "1. **Authenticity**: Does this seem like a genuine customer experience?\n",
    "2. **Specificity**: Does it provide specific details about food, service, or atmosphere?\n",
    "3. **Helpfulness**: Would this review help other customers make decisions?\n",
    "4. **Balance**: Does it provide constructive feedback rather than just complaints?\n",
    "\n",
    "Respond with JSON:\n",
    "{{\n",
    "    \"quality_score\": 0.0-1.0,\n",
    "    \"authenticity\": 0.0-1.0,\n",
    "    \"specificity\": 0.0-1.0,\n",
    "    \"helpfulness\": 0.0-1.0,\n",
    "    \"is_spam\": true/false,\n",
    "    \"is_fake\": true/false,\n",
    "    \"key_insights\": [\"insight1\", \"insight2\"],\n",
    "    \"recommendation\": \"keep\" or \"flag\" or \"remove\"\n",
    "}}\n",
    "\n",
    "Response:\"\"\"\n",
    "        \n",
    "        return prompt\n",
    "    \n",
    "    def classify_review(self, review_text: str, max_retries: int = 3) -> Dict:\n",
    "        \"\"\"Classify a review for policy violations and quality\"\"\"\n",
    "        \n",
    "        if not self.client:\n",
    "            return {\n",
    "                \"error\": \"Model not available\",\n",
    "                \"fallback\": \"Using rule-based detection only\"\n",
    "            }\n",
    "        \n",
    "        if not review_text or len(review_text.strip()) < 5:\n",
    "            return {\n",
    "                \"error\": \"Review text too short\",\n",
    "                \"is_violation\": False,\n",
    "                \"quality_score\": 0.0\n",
    "            }\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        # Policy violation detection\n",
    "        try:\n",
    "            policy_prompt = self.create_policy_prompt(review_text)\n",
    "            \n",
    "            for attempt in range(max_retries):\n",
    "                try:\n",
    "                    policy_response = self.client.text_generation(\n",
    "                        policy_prompt,\n",
    "                        max_new_tokens=200,\n",
    "                        temperature=0.1,\n",
    "                        do_sample=True,\n",
    "                        return_full_text=False\n",
    "                    )\n",
    "                    \n",
    "                    # Parse JSON response\n",
    "                    policy_data = self._parse_json_response(policy_response)\n",
    "                    if policy_data:\n",
    "                        results['policy'] = policy_data\n",
    "                        break\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    if attempt == max_retries - 1:\n",
    "                        results['policy_error'] = str(e)\n",
    "                    time.sleep(1)\n",
    "        \n",
    "        except Exception as e:\n",
    "            results['policy_error'] = str(e)\n",
    "        \n",
    "        # Quality assessment\n",
    "        try:\n",
    "            quality_prompt = self.create_quality_prompt(review_text)\n",
    "            \n",
    "            for attempt in range(max_retries):\n",
    "                try:\n",
    "                    quality_response = self.client.text_generation(\n",
    "                        quality_prompt,\n",
    "                        max_new_tokens=200,\n",
    "                        temperature=0.1,\n",
    "                        do_sample=True,\n",
    "                        return_full_text=False\n",
    "                    )\n",
    "                    \n",
    "                    # Parse JSON response\n",
    "                    quality_data = self._parse_json_response(quality_response)\n",
    "                    if quality_data:\n",
    "                        results['quality'] = quality_data\n",
    "                        break\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    if attempt == max_retries - 1:\n",
    "                        results['quality_error'] = str(e)\n",
    "                    time.sleep(1)\n",
    "        \n",
    "        except Exception as e:\n",
    "            results['quality_error'] = str(e)\n",
    "        \n",
    "        return self._consolidate_results(results)\n",
    "    \n",
    "    def _parse_json_response(self, response: str) -> Optional[Dict]:\n",
    "        \"\"\"Parse JSON from model response\"\"\"\n",
    "        try:\n",
    "            # Find JSON in response\n",
    "            start_idx = response.find('{')\n",
    "            end_idx = response.rfind('}') + 1\n",
    "            \n",
    "            if start_idx != -1 and end_idx != -1:\n",
    "                json_str = response[start_idx:end_idx]\n",
    "                return json.loads(json_str)\n",
    "            \n",
    "        except (json.JSONDecodeError, ValueError) as e:\n",
    "            print(f\"JSON parsing error: {e}\")\n",
    "            print(f\"Response: {response[:200]}...\")\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def _consolidate_results(self, results: Dict) -> Dict:\n",
    "        \"\"\"Consolidate policy and quality results\"\"\"\n",
    "        \n",
    "        consolidated = {\n",
    "            'timestamp': time.time(),\n",
    "            'model_used': self.model_name\n",
    "        }\n",
    "        \n",
    "        # Policy results\n",
    "        if 'policy' in results:\n",
    "            policy = results['policy']\n",
    "            consolidated.update({\n",
    "                'is_violation': policy.get('is_violation', False),\n",
    "                'violation_type': policy.get('violation_type'),\n",
    "                'policy_confidence': policy.get('confidence', 0.0),\n",
    "                'is_trustworthy': policy.get('is_trustworthy', True),\n",
    "                'sentiment': policy.get('sentiment', 'neutral'),\n",
    "                'policy_reasoning': policy.get('reasoning', '')\n",
    "            })\n",
    "        else:\n",
    "            consolidated.update({\n",
    "                'is_violation': False,\n",
    "                'violation_type': None,\n",
    "                'policy_confidence': 0.0,\n",
    "                'policy_error': results.get('policy_error', 'Unknown error')\n",
    "            })\n",
    "        \n",
    "        # Quality results\n",
    "        if 'quality' in results:\n",
    "            quality = results['quality']\n",
    "            consolidated.update({\n",
    "                'quality_score': quality.get('quality_score', 0.5),\n",
    "                'authenticity': quality.get('authenticity', 0.5),\n",
    "                'specificity': quality.get('specificity', 0.5),\n",
    "                'helpfulness': quality.get('helpfulness', 0.5),\n",
    "                'is_spam': quality.get('is_spam', False),\n",
    "                'is_fake': quality.get('is_fake', False),\n",
    "                'key_insights': quality.get('key_insights', []),\n",
    "                'recommendation': quality.get('recommendation', 'keep')\n",
    "            })\n",
    "        else:\n",
    "            consolidated.update({\n",
    "                'quality_score': 0.5,\n",
    "                'quality_error': results.get('quality_error', 'Unknown error')\n",
    "            })\n",
    "        \n",
    "        return consolidated\n",
    "    \n",
    "    def batch_classify(self, reviews: List[str], batch_size: int = 5) -> List[Dict]:\n",
    "        \"\"\"Classify multiple reviews in batches\"\"\"\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        for i in range(0, len(reviews), batch_size):\n",
    "            batch = reviews[i:i + batch_size]\n",
    "            print(f\"🔄 Processing batch {i//batch_size + 1}/{(len(reviews)-1)//batch_size + 1}\")\n",
    "            \n",
    "            batch_results = []\n",
    "            for review in batch:\n",
    "                result = self.classify_review(review)\n",
    "                batch_results.append(result)\n",
    "                time.sleep(0.5)  # Rate limiting\n",
    "            \n",
    "            results.extend(batch_results)\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Initialize Gemma classifier\n",
    "print(\"🚀 Initializing Gemma 3 12B Classifier...\")\n",
    "print(\"💡 Note: You may need a HuggingFace token for full functionality\")\n",
    "\n",
    "# For Colab users, uncomment and add your token:\n",
    "# gemma_classifier = GemmaReviewClassifier(hf_token=\"your_hf_token_here\")\n",
    "\n",
    "# For token-less testing:\n",
    "try:\n",
    "    gemma_classifier = GemmaReviewClassifier()\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Could not initialize Gemma model: {e}\")\n",
    "    print(\"🔄 Continuing with rule-based detection only...\")\n",
    "    gemma_classifier = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2c4fcfc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "🤖 SAMPLE REVIEW CLASSIFICATION - 100 REVIEWS\n",
      "======================================================================\n",
      "🧪 Testing JSONL parsing on your review-other.json file...\n",
      "✅ Successfully parsed sample data:\n",
      "📊 Shape: (5, 8)\n",
      "📋 Original columns: ['user_id', 'name', 'time', 'rating', 'text', 'pics', 'resp', 'gmap_id']\n",
      "🧹 Applying Kaggle-style cleaning to DataFrame with columns: ['user_id', 'name', 'time', 'rating', 'text', 'pics', 'resp', 'gmap_id']\n",
      "🧹 Cleaned dataset: 5 → 5 rows (removed 0)\n",
      "✅ Cleaned sample:\n",
      "📊 Shape: (5, 6)\n",
      "📋 Columns: ['text', 'rating', 'business_name', 'author_name', 'photo', 'rating_category']\n",
      "🔍 First few rows:\n",
      "                                                text  rating    business_name  \\\n",
      "0  Andrea is amazing. Our dog loves her and she a...       5  Amber Thibeault   \n",
      "1  Andrea does a wonderful  job  with our wild Pr...       5           Esther   \n",
      "2                                  Never called back       1      Bob Barrett   \n",
      "\n",
      "  author_name photo rating_category  \n",
      "0      user_0  <NA>            <NA>  \n",
      "1      user_1  <NA>            <NA>  \n",
      "2      user_2  <NA>            <NA>  \n",
      "📂 Searching for additional review datasets...\n",
      "🔄 Attempting to load ./review-other.json...\n",
      "📄 Detected JSONL format (JSON Lines) - parsing line by line...\n",
      "✅ Successfully parsed 162952 JSON objects from JSONL file\n",
      "✅ Created DataFrame with 162952 rows and 8 columns\n",
      "📋 Original columns: ['user_id', 'name', 'time', 'rating', 'text', 'pics', 'resp', 'gmap_id']\n",
      "🧹 Applying Kaggle-style cleaning to DataFrame with columns: ['user_id', 'name', 'time', 'rating', 'text', 'pics', 'resp', 'gmap_id']\n",
      "🧹 Cleaned dataset: 162952 → 105249 rows (removed 57703)\n",
      "✅ Found table with 52 rows from ./Google Local review data.html\n",
      "🧹 Applying Kaggle-style cleaning to DataFrame with columns: [0, 1, 2]\n",
      "❌ Error during cleaning: 'int' object has no attribute 'lower'\n",
      "🔄 Merging 1 additional datasets with Kaggle data...\n",
      "   Adding dataset 1/1 with 145320 rows\n",
      "📊 Merged dataset statistics:\n",
      "   Total reviews: 95495\n",
      "   Kaggle reviews: 1100\n",
      "   Additional reviews: 94395\n",
      "   Duplicates removed: 50925\n",
      "🎯 Sampling 100 reviews from 95495 total reviews...\n",
      "📊 Final sample size: 100 reviews\n",
      "📋 Final columns: ['business_name', 'author_name', 'text', 'rating', 'photo', 'rating_category']\n",
      "📝 Using 'text' column for review text\n",
      "\n",
      "📊 SAMPLE DATASET INFORMATION:\n",
      "   Total reviews for processing: 100\n",
      "   Columns: ['business_name', 'author_name', 'text', 'rating', 'photo', 'rating_category']\n",
      "   Rating distribution: {1.0: np.int64(4), 2.0: np.int64(1), 3.0: np.int64(6), 4.0: np.int64(6), 5.0: np.int64(83)}\n",
      "\n",
      "🔍 Sample of data:\n",
      "   1. Rating: 5.0 | Text: Awsome gifts\n",
      "   2. Rating: 5.0 | Text: Love the history and experience!\n",
      "   3. Rating: 5.0 | Text: Great company for people that are really trying to get their life together..\n",
      "   4. Rating: 5.0 | Text: I live in Georgia but own a property in Northeast Illinois. Ryan worked with me from a distance with...\n",
      "   5. Rating: 5.0 | Text: Excellent quality and the price isn’t too bad.\n",
      "\n",
      "🔄 Processing 100 reviews with Gemma classifier...\n",
      "🚨 RESULTS (Text + Labels):\n",
      "------------------------------------------------------------\n",
      "\n",
      "📝 Review #1: LEGITIMATE\n",
      "⭐ Rating: 5.0\n",
      "🎯 Confidence: 0.000\n",
      "📄 Text: \"Awsome gifts\"\n",
      "\n",
      "📝 Review #2: LEGITIMATE\n",
      "⭐ Rating: 5.0\n",
      "🎯 Confidence: 0.000\n",
      "📄 Text: \"Love the history and experience!\"\n",
      "\n",
      "📝 Review #3: LEGITIMATE\n",
      "⭐ Rating: 5.0\n",
      "🎯 Confidence: 0.000\n",
      "📄 Text: \"Great company for people that are really trying to get their life together..\"\n",
      "\n",
      "📝 Review #4: LEGITIMATE\n",
      "⭐ Rating: 5.0\n",
      "🎯 Confidence: 0.000\n",
      "📄 Text: \"I live in Georgia but own a property in Northeast Illinois. Ryan worked with me from a distance with a mouse problem we were having. He was reliable, ...\"\n",
      "\n",
      "📝 Review #5: LEGITIMATE\n",
      "⭐ Rating: 5.0\n",
      "🎯 Confidence: 0.000\n",
      "📄 Text: \"Excellent quality and the price isn’t too bad.\"\n",
      "\n",
      "📝 Review #6: LEGITIMATE\n",
      "⭐ Rating: 3.0\n",
      "🎯 Confidence: 0.000\n",
      "📄 Text: \"Decent selection of food. Staff was sort of friendly. I don't remember seeing my waiter much. It was an interesting experience to say the least. Food ...\"\n",
      "\n",
      "📝 Review #7: LEGITIMATE\n",
      "⭐ Rating: 5.0\n",
      "🎯 Confidence: 0.000\n",
      "📄 Text: \"Incredible customer service. I’ve ordered a couple of swimsuits and dresses from this site. I recently ordered a jumper for my daughter and I to match...\"\n",
      "\n",
      "📝 Review #8: LEGITIMATE\n",
      "⭐ Rating: 5.0\n",
      "🎯 Confidence: 0.000\n",
      "📄 Text: \"Cool little coffee shop!\"\n",
      "\n",
      "📝 Review #9: LEGITIMATE\n",
      "⭐ Rating: 5.0\n",
      "🎯 Confidence: 0.000\n",
      "📄 Text: \"A less visited sight,  but very neat. If you are walking from the Washington monument over to Lincoln, I'd recommend it. It's lovely, we had a small l...\"\n",
      "\n",
      "📊 Progress: 10/100 reviews\n",
      "------------------------------------------------------------\n",
      "\n",
      "📝 Review #10: LEGITIMATE\n",
      "⭐ Rating: 5.0\n",
      "🎯 Confidence: 0.000\n",
      "📄 Text: \"The clothes are well- made. Especially the ones I bought that are for winter ( very thick and has 80% cotton) This is my first time buying from them a...\"\n",
      "\n",
      "📝 Review #11: LEGITIMATE\n",
      "⭐ Rating: 5.0\n",
      "🎯 Confidence: 0.000\n",
      "📄 Text: \"Awesome experience to work with\n",
      "Would do it again\n",
      "\n",
      "And the character did amazing everyone loved him\"\n",
      "\n",
      "📝 Review #12: LEGITIMATE\n",
      "⭐ Rating: 4.0\n",
      "🎯 Confidence: 0.000\n",
      "📄 Text: \"(Translated by Google) Lots of variety of products and great price. To highlight above all the excellent customer service (in our case from the hand o...\"\n",
      "\n",
      "📝 Review #13: LEGITIMATE\n",
      "⭐ Rating: 5.0\n",
      "🎯 Confidence: 0.000\n",
      "📄 Text: \"Easy to book online. Quick response. Included a free (download) guide to Rome. I look forward to traveling on ItaliaRail.\"\n",
      "\n",
      "📝 Review #14: LEGITIMATE\n",
      "⭐ Rating: 5.0\n",
      "🎯 Confidence: 0.000\n",
      "📄 Text: \"Needed some advice on how to insulate my commercial roll up door and they were right on it!  Very helpful!\"\n",
      "\n",
      "📝 Review #15: LEGITIMATE\n",
      "⭐ Rating: 1.0\n",
      "🎯 Confidence: 0.000\n",
      "📄 Text: \"Yeah the food is not that great and it takes forever to arrive. They say 25 minutes but you may wait until the 2nd coming, or until you hit menopause....\"\n",
      "\n",
      "📝 Review #16: LEGITIMATE\n",
      "⭐ Rating: 5.0\n",
      "🎯 Confidence: 0.000\n",
      "📄 Text: \"Wonderful menu, excellent service, and who doesn't love a glass waterfall?\"\n",
      "\n",
      "📝 Review #17: LEGITIMATE\n",
      "⭐ Rating: 5.0\n",
      "🎯 Confidence: 0.000\n",
      "📄 Text: \"Very polite and quick solution of my request😉👍\"\n",
      "\n",
      "📝 Review #18: LEGITIMATE\n",
      "⭐ Rating: 5.0\n",
      "🎯 Confidence: 0.000\n",
      "📄 Text: \"Used Bellhop for an out of state move to D.C. After getting quotes and reading reviews for 4 companies, Bellhop was the most affordable and most consi...\"\n",
      "\n",
      "📝 Review #19: LEGITIMATE\n",
      "⭐ Rating: 5.0\n",
      "🎯 Confidence: 0.000\n",
      "📄 Text: \"Shakari led us through historical and geographical insights about the recipe we were cooking and then easily led us into the preparation of the meal, ...\"\n",
      "\n",
      "📊 Progress: 20/100 reviews\n",
      "------------------------------------------------------------\n",
      "\n",
      "📝 Review #20: LEGITIMATE\n",
      "⭐ Rating: 5.0\n",
      "🎯 Confidence: 0.000\n",
      "📄 Text: \"First I want to say that the assistance and support I received, particularly as a new customer from ADM and from the owner and founder, Mr. Anthony Da...\"\n",
      "\n",
      "📝 Review #21: LEGITIMATE\n",
      "⭐ Rating: 5.0\n",
      "🎯 Confidence: 0.000\n",
      "📄 Text: \"The best place in DC by far.  It's charming , beautiful gardens and walkways.\"\n",
      "\n",
      "📝 Review #22: LEGITIMATE\n",
      "⭐ Rating: 5.0\n",
      "🎯 Confidence: 0.000\n",
      "📄 Text: \"Amazing place!!!\"\n",
      "\n",
      "📝 Review #23: LEGITIMATE\n",
      "⭐ Rating: 5.0\n",
      "🎯 Confidence: 0.000\n",
      "📄 Text: \"Historical place\"\n",
      "\n",
      "📝 Review #24: LEGITIMATE\n",
      "⭐ Rating: 5.0\n",
      "🎯 Confidence: 0.000\n",
      "📄 Text: \"Great service. Use weekly\"\n",
      "\n",
      "📝 Review #25: LEGITIMATE\n",
      "⭐ Rating: 4.0\n",
      "🎯 Confidence: 0.000\n",
      "📄 Text: \"Best udon in Paris. Well Korean celebrities visits it too. Just a little bit crowded.\"\n",
      "\n",
      "📝 Review #26: LEGITIMATE\n",
      "⭐ Rating: 5.0\n",
      "🎯 Confidence: 0.000\n",
      "📄 Text: \"Susie B was so helpful with my exchange. She was pleasant, efficient and fast in her response. I love PatPat clothes for my grand babies. The clothes ...\"\n",
      "\n",
      "📝 Review #27: LEGITIMATE\n",
      "⭐ Rating: 5.0\n",
      "🎯 Confidence: 0.000\n",
      "📄 Text: \"Hockey!!!!!!\"\n",
      "\n",
      "📝 Review #28: LEGITIMATE\n",
      "⭐ Rating: 5.0\n",
      "🎯 Confidence: 0.000\n",
      "📄 Text: \"Easy order. Took a little bit to get but the communication was great. When I received my order I was thrilled. 3 custom necklaces all perfect and I lo...\"\n",
      "\n",
      "📝 Review #29: LEGITIMATE\n",
      "⭐ Rating: 5.0\n",
      "🎯 Confidence: 0.000\n",
      "📄 Text: \"I love it, it was crowded\"\n",
      "\n",
      "📊 Progress: 30/100 reviews\n",
      "------------------------------------------------------------\n",
      "\n",
      "📝 Review #30: LEGITIMATE\n",
      "⭐ Rating: 5.0\n",
      "🎯 Confidence: 0.000\n",
      "📄 Text: \"Jonny Wu is amazing! Best show ever!\"\n",
      "\n",
      "📝 Review #31: LEGITIMATE\n",
      "⭐ Rating: 5.0\n",
      "🎯 Confidence: 0.000\n",
      "📄 Text: \"Love the view.  Very unique\"\n",
      "\n",
      "📝 Review #32: LEGITIMATE\n",
      "⭐ Rating: 5.0\n",
      "🎯 Confidence: 0.000\n",
      "📄 Text: \"I found everything I needed. Staff were helpful and considerate!!\"\n",
      "\n",
      "📝 Review #33: LEGITIMATE\n",
      "⭐ Rating: 5.0\n",
      "🎯 Confidence: 0.000\n",
      "📄 Text: \"Their work is amazing!!! Thank you!!\"\n",
      "\n",
      "📝 Review #34: LEGITIMATE\n",
      "⭐ Rating: 5.0\n",
      "🎯 Confidence: 0.000\n",
      "📄 Text: \"fast and friendly no scams here removed the account that i forgot the info for   it and now its good to use..\"\n",
      "\n",
      "📝 Review #35: LEGITIMATE\n",
      "⭐ Rating: 5.0\n",
      "🎯 Confidence: 0.000\n",
      "📄 Text: \"They went out of their way to help me with my car out of their usual area of service on New YEARS Eve! I am so grateful and pleased with the service t...\"\n",
      "\n",
      "📝 Review #36: LEGITIMATE\n",
      "⭐ Rating: 5.0\n",
      "🎯 Confidence: 0.000\n",
      "📄 Text: \"Clyde's has been a favorite of mine for years.\"\n",
      "\n",
      "📝 Review #37: LEGITIMATE\n",
      "⭐ Rating: 5.0\n",
      "🎯 Confidence: 0.000\n",
      "📄 Text: \"Very good training. The devil is in the detail and Bradley goes into great detail at a wonderful pace. Thoroughly recommend!\"\n",
      "\n",
      "📝 Review #38: LEGITIMATE\n",
      "⭐ Rating: 5.0\n",
      "🎯 Confidence: 0.000\n",
      "📄 Text: \"This guys are great! I called with an Amazon Issue and they told me to not hire anybody yet and to not spend money in something that could affect me. ...\"\n",
      "\n",
      "📝 Review #39: LEGITIMATE\n",
      "⭐ Rating: 5.0\n",
      "🎯 Confidence: 0.000\n",
      "📄 Text: \"This was the first time using movers and I'm glad I went with Bellhops! Yann, Nicholas, and Joshua were quick and efficient. They really hustled even ...\"\n",
      "\n",
      "📊 Progress: 40/100 reviews\n",
      "------------------------------------------------------------\n",
      "\n",
      "📝 Review #40: LEGITIMATE\n",
      "⭐ Rating: 5.0\n",
      "🎯 Confidence: 0.000\n",
      "📄 Text: \"I'm so glad I came upon this site. They had so many options and were flexible in their wording options. I decided not to pay for some of the embellish...\"\n",
      "\n",
      "📝 Review #41: LEGITIMATE\n",
      "⭐ Rating: 5.0\n",
      "🎯 Confidence: 0.000\n",
      "📄 Text: \"Pretty cool place to watch the planes take off. Free parking. Toilets on site are dirty. Use the bathroom before you go here.\"\n",
      "\n",
      "📝 Review #42: LEGITIMATE\n",
      "⭐ Rating: 5.0\n",
      "🎯 Confidence: 0.000\n",
      "📄 Text: \"Hazel provides excellent customer services.\"\n",
      "\n",
      "📝 Review #43: LEGITIMATE\n",
      "⭐ Rating: 4.0\n",
      "🎯 Confidence: 0.000\n",
      "📄 Text: \"Good amount of food choices, seating is nice\"\n",
      "\n",
      "📝 Review #44: LEGITIMATE\n",
      "⭐ Rating: 5.0\n",
      "🎯 Confidence: 0.000\n",
      "📄 Text: \"Jerell and his team were super awesome! I'm new to lawn care and landscaping and Jerell was super patient in answering all of my questions (regardless...\"\n",
      "\n",
      "📝 Review #45: LEGITIMATE\n",
      "⭐ Rating: 5.0\n",
      "🎯 Confidence: 0.000\n",
      "📄 Text: \"Had a great time at Prayer March 2020\"\n",
      "\n",
      "📝 Review #46: LEGITIMATE\n",
      "⭐ Rating: 5.0\n",
      "🎯 Confidence: 0.000\n",
      "📄 Text: \"Wasn’t sure if it was legit because I couldn’t find that many reviews but rest assured they got the goods and I am so glad :)\"\n",
      "\n",
      "📝 Review #47: LEGITIMATE\n",
      "⭐ Rating: 5.0\n",
      "🎯 Confidence: 0.000\n",
      "📄 Text: \"Had a great experience with Air Control LLC. The technician was running late b/c of traffic and was super apologetic when he arrives. Overall, the tec...\"\n",
      "\n",
      "📝 Review #48: LEGITIMATE\n",
      "⭐ Rating: 3.0\n",
      "🎯 Confidence: 0.000\n",
      "📄 Text: \"(Translated by Google) Nicely at Potomac\n",
      "\n",
      "(Original)\n",
      "Pent ved Potomac\"\n",
      "\n",
      "📝 Review #49: LEGITIMATE\n",
      "⭐ Rating: 4.0\n",
      "🎯 Confidence: 0.000\n",
      "📄 Text: \"Very fast and helpful customer service. Jayson was friendly and efficient in handling my return items. I bought a suze to big. I will be but I more fo...\"\n",
      "\n",
      "📊 Progress: 50/100 reviews\n",
      "------------------------------------------------------------\n",
      "\n",
      "📝 Review #50: LEGITIMATE\n",
      "⭐ Rating: 5.0\n",
      "🎯 Confidence: 0.000\n",
      "📄 Text: \"Nick is very thorough  and honest\n",
      "Thanks for a great  inspection.\"\n",
      "\n",
      "📝 Review #51: LEGITIMATE\n",
      "⭐ Rating: 2.0\n",
      "🎯 Confidence: 0.000\n",
      "📄 Text: \"On my 3rd device in just a little over a month... first one didn’t hit right every time and i exchanged it where i got it. The second lasted a little ...\"\n",
      "\n",
      "📝 Review #52: LEGITIMATE\n",
      "⭐ Rating: 5.0\n",
      "🎯 Confidence: 0.000\n",
      "📄 Text: \"The absolute best show ever.  We want to see him again and again.\"\n",
      "\n",
      "📝 Review #53: LEGITIMATE\n",
      "⭐ Rating: 5.0\n",
      "🎯 Confidence: 0.000\n",
      "📄 Text: \"Great service and smooth process for ordering a new surfboard. For a long time I was looking for a store online where I can purchase my first board, b...\"\n",
      "\n",
      "📝 Review #54: LEGITIMATE\n",
      "⭐ Rating: 5.0\n",
      "🎯 Confidence: 0.000\n",
      "📄 Text: \"Super happy with these guys. I've wanted to join them for many years and now that I have, I couldn't imagine going anywhere else. They are like your m...\"\n",
      "\n",
      "📝 Review #55: LEGITIMATE\n",
      "⭐ Rating: 4.0\n",
      "🎯 Confidence: 0.000\n",
      "📄 Text: \"Very good selection of dishes insensible portion sizes.\"\n",
      "\n",
      "📝 Review #56: LEGITIMATE\n",
      "⭐ Rating: 5.0\n",
      "🎯 Confidence: 0.000\n",
      "📄 Text: \"I have recently purchased matching Christmas pyjamas for my whole family from this site, have to be honest was a little sceptical about products but w...\"\n",
      "\n",
      "📝 Review #57: LEGITIMATE\n",
      "⭐ Rating: 5.0\n",
      "🎯 Confidence: 0.000\n",
      "📄 Text: \"Good results. Price was very reasonable. Will recommend to others.\"\n",
      "\n",
      "📝 Review #58: LEGITIMATE\n",
      "⭐ Rating: 4.0\n",
      "🎯 Confidence: 0.000\n",
      "📄 Text: \"Better version of Chipotle.\"\n",
      "\n",
      "📝 Review #59: LEGITIMATE\n",
      "⭐ Rating: 5.0\n",
      "🎯 Confidence: 0.000\n",
      "📄 Text: \"I’ve used this place multiple times and it is always a breeze. Exactly what you ask for. They are very good, fast, and reliable. U will use them again...\"\n",
      "\n",
      "📊 Progress: 60/100 reviews\n",
      "------------------------------------------------------------\n",
      "\n",
      "📝 Review #60: LEGITIMATE\n",
      "⭐ Rating: 5.0\n",
      "🎯 Confidence: 0.000\n",
      "📄 Text: \"great product.  Love the erformance\"\n",
      "\n",
      "📝 Review #61: LEGITIMATE\n",
      "⭐ Rating: 5.0\n",
      "🎯 Confidence: 0.000\n",
      "📄 Text: \"I called 2 days before our event and they where able to provide me with what I needed. Customer service was awesome as well and I would definitely use...\"\n",
      "\n",
      "📝 Review #62: LEGITIMATE\n",
      "⭐ Rating: 5.0\n",
      "🎯 Confidence: 0.000\n",
      "📄 Text: \"Good customer Service. Rachel solved our request promptly.\"\n",
      "\n",
      "📝 Review #63: LEGITIMATE\n",
      "⭐ Rating: 5.0\n",
      "🎯 Confidence: 0.000\n",
      "📄 Text: \"A little slower than I expected, but the customer service was great! Order shipped within a week.\"\n",
      "\n",
      "📝 Review #64: LEGITIMATE\n",
      "⭐ Rating: 5.0\n",
      "🎯 Confidence: 0.000\n",
      "📄 Text: \"Increased my business into a lot of customers. Thankfully we found them saved my business. 🤗🤩\"\n",
      "\n",
      "📝 Review #65: LEGITIMATE\n",
      "⭐ Rating: 5.0\n",
      "🎯 Confidence: 0.000\n",
      "📄 Text: \"Really gorgeous decor and ambient was magnificent.  Bathroom on the second floor with a crazy loopy stairs a big minus but there's an elevator.  Resta...\"\n",
      "\n",
      "📝 Review #66: LEGITIMATE\n",
      "⭐ Rating: 5.0\n",
      "🎯 Confidence: 0.000\n",
      "📄 Text: \"It is a very well kept site. I was honored to pay my respect to President Lincoln in visiting the home where he passed away.\"\n",
      "\n",
      "📝 Review #67: LEGITIMATE\n",
      "⭐ Rating: 5.0\n",
      "🎯 Confidence: 0.000\n",
      "📄 Text: \"On time, professional, expert and quick.\"\n",
      "\n",
      "📝 Review #68: LEGITIMATE\n",
      "⭐ Rating: 5.0\n",
      "🎯 Confidence: 0.000\n",
      "📄 Text: \"Excellent work.  They are extremely nice and  friendly.  They are also accommodating with schedules for busy families.\"\n",
      "\n",
      "📝 Review #69: LEGITIMATE\n",
      "⭐ Rating: 5.0\n",
      "🎯 Confidence: 0.000\n",
      "📄 Text: \"(Translated by Google) Quality\n",
      "\n",
      "(Original)\n",
      "Calidad\"\n",
      "\n",
      "📊 Progress: 70/100 reviews\n",
      "------------------------------------------------------------\n",
      "\n",
      "📝 Review #70: LEGITIMATE\n",
      "⭐ Rating: 1.0\n",
      "🎯 Confidence: 0.000\n",
      "📄 Text: \"I had ordered in January and the payment has been done too..but still I haven't received my parcel.. People Please don't take risk buying here...they ...\"\n",
      "\n",
      "📝 Review #71: LEGITIMATE\n",
      "⭐ Rating: 5.0\n",
      "🎯 Confidence: 0.000\n",
      "📄 Text: \"Thoroughly enjoyed the Crimes and Scandals Tour of Embassy Row with Rebecca! Learned a ton of interesting facts and enjoyed her awesome personality. C...\"\n",
      "\n",
      "📝 Review #72: LEGITIMATE\n",
      "⭐ Rating: 1.0\n",
      "🎯 Confidence: 0.000\n",
      "📄 Text: \"I normally don't rate any business very harshly but this one deserves it. I will let others make their own judgement to visit or not. I did visit base...\"\n",
      "\n",
      "📝 Review #73: LEGITIMATE\n",
      "⭐ Rating: 3.0\n",
      "🎯 Confidence: 0.000\n",
      "📄 Text: \"Very expensive and not a lot to offer\"\n",
      "\n",
      "📝 Review #74: LEGITIMATE\n",
      "⭐ Rating: 5.0\n",
      "🎯 Confidence: 0.000\n",
      "📄 Text: \"(Translated by Google) Tasty tasty!!\n",
      "\n",
      "(Original)\n",
      "Rico rico!!\"\n",
      "\n",
      "📝 Review #75: LEGITIMATE\n",
      "⭐ Rating: 5.0\n",
      "🎯 Confidence: 0.000\n",
      "📄 Text: \"Absolutely amazing. Has helped me to grow as a new artist, person, and woman. I am immensely grateful for your love for art and teaching, who you are,...\"\n",
      "\n",
      "📝 Review #76: LEGITIMATE\n",
      "⭐ Rating: 5.0\n",
      "🎯 Confidence: 0.000\n",
      "📄 Text: \"The software is incredibly easy use, even as someone that does not have any experience.  The team is there to support you if needed but have designed ...\"\n",
      "\n",
      "📝 Review #77: LEGITIMATE\n",
      "⭐ Rating: 5.0\n",
      "🎯 Confidence: 0.000\n",
      "📄 Text: \"The product was exceptional!! The shipping was so fast I was shocked to see it on my doorstep. Thank you!!\"\n",
      "\n",
      "📝 Review #78: LEGITIMATE\n",
      "⭐ Rating: 5.0\n",
      "🎯 Confidence: 0.000\n",
      "📄 Text: \"Awesome food. CJ is the man. Get greens and the brisket!\"\n",
      "\n",
      "📝 Review #79: LEGITIMATE\n",
      "⭐ Rating: 5.0\n",
      "🎯 Confidence: 0.000\n",
      "📄 Text: \"Amazing costumer service! I ordered a few parts that were lost in transit. They took care of the problem right away. Which we actually received the or...\"\n",
      "\n",
      "📊 Progress: 80/100 reviews\n",
      "------------------------------------------------------------\n",
      "\n",
      "📝 Review #80: LEGITIMATE\n",
      "⭐ Rating: 5.0\n",
      "🎯 Confidence: 0.000\n",
      "📄 Text: \"As Always I Love This Sight For My Baby I’m Always Ordering Clothes... I Wanted To Get The Matching Christmas Pajamas For Pictures And What I Ordered ...\"\n",
      "\n",
      "📝 Review #81: LEGITIMATE\n",
      "⭐ Rating: 5.0\n",
      "🎯 Confidence: 0.000\n",
      "📄 Text: \"very convenient for people to have a place to work with other business partners.   great to know this type of hotel to stay. will use this service mor...\"\n",
      "\n",
      "📝 Review #82: LEGITIMATE\n",
      "⭐ Rating: 5.0\n",
      "🎯 Confidence: 0.000\n",
      "📄 Text: \"Awesome tacos\"\n",
      "\n",
      "📝 Review #83: LEGITIMATE\n",
      "⭐ Rating: 5.0\n",
      "🎯 Confidence: 0.000\n",
      "📄 Text: \"Highly recommend, real fun virtual team outing!\"\n",
      "\n",
      "📝 Review #84: LEGITIMATE\n",
      "⭐ Rating: 5.0\n",
      "🎯 Confidence: 0.000\n",
      "📄 Text: \"They are the best source of information for all things Amazon!  After a full year of trying to get my Amazon account back.  I called them last weeek a...\"\n",
      "\n",
      "📝 Review #85: LEGITIMATE\n",
      "⭐ Rating: 1.0\n",
      "🎯 Confidence: 0.000\n",
      "📄 Text: \"I just had a really bad experience with this eatery! Ordered food from here and the jollof rice arrived dead cold,  stale, and hopelessly unappealing!...\"\n",
      "\n",
      "📝 Review #86: LEGITIMATE\n",
      "⭐ Rating: 5.0\n",
      "🎯 Confidence: 0.000\n",
      "📄 Text: \"I did the White House at Night Scandals tour. Our guide Boglarka was great! She was really funny and knowledgeable. She was open to questions. She gav...\"\n",
      "\n",
      "📝 Review #87: LEGITIMATE\n",
      "⭐ Rating: 5.0\n",
      "🎯 Confidence: 0.000\n",
      "📄 Text: \"This was overall a great experience. I came across Jungle Blunts on YouTube. I then placed a order and received them a lot soon these expected. I proc...\"\n",
      "\n",
      "📝 Review #88: LEGITIMATE\n",
      "⭐ Rating: 3.0\n",
      "🎯 Confidence: 0.000\n",
      "📄 Text: \"It's ok, but huge lines.\"\n",
      "\n",
      "📝 Review #89: LEGITIMATE\n",
      "⭐ Rating: 5.0\n",
      "🎯 Confidence: 0.000\n",
      "📄 Text: \"This is the spot for all of your surfing needs.  They have all the selection, the best prices and are just cool to chat with. Board cave is also build...\"\n",
      "\n",
      "📊 Progress: 90/100 reviews\n",
      "------------------------------------------------------------\n",
      "\n",
      "📝 Review #90: LEGITIMATE\n",
      "⭐ Rating: 5.0\n",
      "🎯 Confidence: 0.000\n",
      "📄 Text: \"Good drinks, good food...not very authentic Thai food though. The customer service was exceptional.\"\n",
      "\n",
      "📝 Review #91: LEGITIMATE\n",
      "⭐ Rating: 5.0\n",
      "🎯 Confidence: 0.000\n",
      "📄 Text: \"Delicious barbecue in a great location! Great quality meats and creative side options. Tons of seating with friendly service. Will definitely be back ...\"\n",
      "\n",
      "📝 Review #92: LEGITIMATE\n",
      "⭐ Rating: 5.0\n",
      "🎯 Confidence: 0.000\n",
      "📄 Text: \"Intimate dining with wonderful small plates representative of the street food found in countries across the world. Really well done.\"\n",
      "\n",
      "📝 Review #93: LEGITIMATE\n",
      "⭐ Rating: 5.0\n",
      "🎯 Confidence: 0.000\n",
      "📄 Text: \"Massive and awesome monument\"\n",
      "\n",
      "📝 Review #94: LEGITIMATE\n",
      "⭐ Rating: 5.0\n",
      "🎯 Confidence: 0.000\n",
      "📄 Text: \"Friendly staff, quick service, cash only, and as you would expect, tasty diner breakfast! Their description says it right, \"no frills!\"\"\n",
      "\n",
      "📝 Review #95: LEGITIMATE\n",
      "⭐ Rating: 5.0\n",
      "🎯 Confidence: 0.000\n",
      "📄 Text: \"Had so much fun and such a great workout!!\"\n",
      "\n",
      "📝 Review #96: LEGITIMATE\n",
      "⭐ Rating: 5.0\n",
      "🎯 Confidence: 0.000\n",
      "📄 Text: \"Food is EXCELLENT and people are awesome.\"\n",
      "\n",
      "📝 Review #97: LEGITIMATE\n",
      "⭐ Rating: 3.0\n",
      "🎯 Confidence: 0.000\n",
      "📄 Text: \"They alright just because\"\n",
      "\n",
      "📝 Review #98: LEGITIMATE\n",
      "⭐ Rating: 5.0\n",
      "🎯 Confidence: 0.000\n",
      "📄 Text: \"Great service, thank you again.\"\n",
      "\n",
      "📝 Review #99: LEGITIMATE\n",
      "⭐ Rating: 5.0\n",
      "🎯 Confidence: 0.000\n",
      "📄 Text: \"(Translated by Google) Thanks Rachel for your good management and great service! I recommend this page to buy, great attention!\n",
      "\n",
      "(Original)\n",
      "Gracias Ra...\"\n",
      "\n",
      "📊 Progress: 100/100 reviews\n",
      "------------------------------------------------------------\n",
      "\n",
      "📝 Review #100: LEGITIMATE\n",
      "⭐ Rating: 3.0\n",
      "🎯 Confidence: 0.000\n",
      "📄 Text: \"Food--AMAZING.  Got the something something sampler, everything DELICIOUS!  Staff was awesome and very knowledgeable and the food was ready in a SNAP!...\"\n",
      "\n",
      "======================================================================\n",
      "📊 PROCESSING COMPLETE!\n",
      "📋 Label distribution:\n",
      "   LEGITIMATE: 100\n",
      "💾 Results saved to: sample_100_reviews_with_labels_1756572492.csv\n",
      "📊 Output columns: ['index', 'text', 'rating', 'label', 'confidence', 'reason']\n",
      "\n",
      "🔍 First 5 results:\n",
      "   index       label  rating\n",
      "0      1  LEGITIMATE     5.0\n",
      "1      2  LEGITIMATE     5.0\n",
      "2      3  LEGITIMATE     5.0\n",
      "3      4  LEGITIMATE     5.0\n",
      "4      5  LEGITIMATE     5.0\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# 🚀 Sample Review Classification - 100 Reviews with Text and Labels\n",
    "print(\"=\" * 70)\n",
    "print(\"🤖 SAMPLE REVIEW CLASSIFICATION - 100 REVIEWS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "def load_additional_reviews():\n",
    "    \"\"\"Load additional review datasets - simplified to extract only text and rating\"\"\"\n",
    "    additional_dfs = []\n",
    "    \n",
    "    # Check for additional review files\n",
    "    additional_files = [\n",
    "        './review-other.json',\n",
    "        './Google Local review data.html',\n",
    "        # Add more file paths as needed\n",
    "    ]\n",
    "    \n",
    "    for file_path in additional_files:\n",
    "        if os.path.exists(file_path):\n",
    "            try:\n",
    "                if file_path.endswith('.json'):\n",
    "                    # Enhanced JSON loading with better error handling\n",
    "                    print(f\"🔄 Attempting to load {file_path}...\")\n",
    "                    \n",
    "                    # First, peek at the file to determine format\n",
    "                    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                        first_line = f.readline().strip()\n",
    "                        second_line = f.readline().strip()\n",
    "                    \n",
    "                    # Check if it's JSONL format (each line is a JSON object)\n",
    "                    if (first_line.startswith('{') and first_line.endswith('}') and \n",
    "                        second_line.startswith('{') and second_line.endswith('}')):\n",
    "                        \n",
    "                        print(f\"📄 Detected JSONL format (JSON Lines) - parsing line by line...\")\n",
    "                        json_objects = []\n",
    "                        \n",
    "                        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                            for line_num, line in enumerate(f, 1):\n",
    "                                line = line.strip()\n",
    "                                if line:  # Skip empty lines\n",
    "                                    try:\n",
    "                                        obj = json.loads(line)\n",
    "                                        json_objects.append(obj)\n",
    "                                    except json.JSONDecodeError as e:\n",
    "                                        print(f\"⚠️ Skipping invalid JSON on line {line_num}: {e}\")\n",
    "                                        continue\n",
    "                        \n",
    "                        json_data = json_objects\n",
    "                        print(f\"✅ Successfully parsed {len(json_data)} JSON objects from JSONL file\")\n",
    "                        \n",
    "                    else:\n",
    "                        # Try standard JSON formats\n",
    "                        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                            content = f.read().strip()\n",
    "                        \n",
    "                        if content.startswith('[') and content.endswith(']'):\n",
    "                            # Array of JSON objects\n",
    "                            json_data = json.loads(content)\n",
    "                            print(f\"✅ Successfully parsed JSON array with {len(json_data)} objects\")\n",
    "                        elif content.startswith('{') and content.endswith('}'):\n",
    "                            # Single JSON object\n",
    "                            json_data = json.loads(content)\n",
    "                            print(f\"✅ Successfully parsed single JSON object\")\n",
    "                        else:\n",
    "                            print(f\"❌ Unrecognized JSON format in {file_path}\")\n",
    "                            continue\n",
    "                    \n",
    "                    # Convert to DataFrame and apply same cleaning as Kaggle data\n",
    "                    if isinstance(json_data, list):\n",
    "                        if json_data:  # Check if list is not empty\n",
    "                            full_df = pd.DataFrame(json_data)\n",
    "                            print(f\"✅ Created DataFrame with {len(full_df)} rows and {len(full_df.columns)} columns\")\n",
    "                            print(f\"📋 Original columns: {list(full_df.columns)}\")\n",
    "                            \n",
    "                            # Apply same cleaning process as Kaggle dataset\n",
    "                            cleaned_df = clean_json_data_like_kaggle(full_df)\n",
    "                            if cleaned_df is not None and len(cleaned_df) > 0:\n",
    "                                additional_dfs.append(cleaned_df)\n",
    "                        else:\n",
    "                            print(f\"⚠️ JSON file {file_path} contains empty array\")\n",
    "                    else:\n",
    "                        full_df = pd.json_normalize(json_data)\n",
    "                        print(f\"✅ Created DataFrame with {len(full_df)} rows and {len(full_df.columns)} columns\")\n",
    "                        cleaned_df = clean_json_data_like_kaggle(full_df)\n",
    "                        if cleaned_df is not None and len(cleaned_df) > 0:\n",
    "                            additional_dfs.append(cleaned_df)\n",
    "                        \n",
    "                elif file_path.endswith('.html'):\n",
    "                    # Enhanced HTML parsing with dependency installation\n",
    "                    try:\n",
    "                        # First try to install lxml if not available\n",
    "                        try:\n",
    "                            import lxml\n",
    "                        except ImportError:\n",
    "                            print(f\"📦 Installing lxml for HTML parsing...\")\n",
    "                            import subprocess\n",
    "                            import sys\n",
    "                            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"lxml\"])\n",
    "                            print(f\"✅ lxml installed successfully\")\n",
    "                        \n",
    "                        # Now try to parse HTML\n",
    "                        html_tables = pd.read_html(file_path, encoding='utf-8')\n",
    "                        if html_tables:\n",
    "                            # If multiple tables, try to find the one with review-like data\n",
    "                            best_table = None\n",
    "                            max_rows = 0\n",
    "                            \n",
    "                            for i, table in enumerate(html_tables):\n",
    "                                if len(table) > max_rows:\n",
    "                                    # Look for text-like columns\n",
    "                                    text_like_cols = [col for col in table.columns if \n",
    "                                                    any(keyword in str(col).lower() for keyword in \n",
    "                                                        ['text', 'review', 'comment', 'content', 'message'])]\n",
    "                                    if text_like_cols or len(table.columns) >= 3:  # Reasonable number of columns\n",
    "                                        best_table = table\n",
    "                                        max_rows = len(table)\n",
    "                            \n",
    "                            if best_table is not None:\n",
    "                                print(f\"✅ Found table with {len(best_table)} rows from {file_path}\")\n",
    "                                cleaned_df = clean_json_data_like_kaggle(best_table)\n",
    "                                if cleaned_df is not None and len(cleaned_df) > 0:\n",
    "                                    additional_dfs.append(cleaned_df)\n",
    "                            else:\n",
    "                                print(f\"⚠️ No suitable table found in {file_path}\")\n",
    "                        else:\n",
    "                            print(f\"⚠️ No tables found in HTML file {file_path}\")\n",
    "                            \n",
    "                    except Exception as e:\n",
    "                        print(f\"❌ Could not parse HTML file {file_path}: {e}\")\n",
    "                        print(f\"💡 Try converting the HTML file to CSV format manually\")\n",
    "                        \n",
    "                elif file_path.endswith('.csv'):\n",
    "                    try:\n",
    "                        # Try different encodings\n",
    "                        encodings = ['utf-8', 'latin-1', 'cp1252', 'iso-8859-1']\n",
    "                        full_df = None\n",
    "                        \n",
    "                        for encoding in encodings:\n",
    "                            try:\n",
    "                                full_df = pd.read_csv(file_path, encoding=encoding)\n",
    "                                print(f\"✅ Loaded {len(full_df)} rows from {file_path} (encoding: {encoding})\")\n",
    "                                break\n",
    "                            except UnicodeDecodeError:\n",
    "                                continue\n",
    "                        \n",
    "                        if full_df is not None:\n",
    "                            cleaned_df = clean_json_data_like_kaggle(full_df)\n",
    "                            if cleaned_df is not None and len(cleaned_df) > 0:\n",
    "                                additional_dfs.append(cleaned_df)\n",
    "                        else:\n",
    "                            print(f\"❌ Could not read CSV file {file_path} with any encoding\")\n",
    "                            \n",
    "                    except Exception as e:\n",
    "                        print(f\"❌ Error loading CSV {file_path}: {e}\")\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"❌ Unexpected error loading {file_path}: {e}\")\n",
    "                import traceback\n",
    "                print(f\"📋 Full error traceback:\")\n",
    "                traceback.print_exc()\n",
    "        else:\n",
    "            print(f\"ℹ️ File not found: {file_path}\")\n",
    "    \n",
    "    return additional_dfs\n",
    "\n",
    "def clean_json_data_like_kaggle(df):\n",
    "    \"\"\"Clean JSON data using the same method as Kaggle dataset (clean_reviews_dataset)\"\"\"\n",
    "    \n",
    "    print(f\"🧹 Applying Kaggle-style cleaning to DataFrame with columns: {list(df.columns)}\")\n",
    "    \n",
    "    try:\n",
    "        # Use the same _find_col function and cleaning logic as Kaggle dataset\n",
    "        col_business = _find_col(df, [\"business_name\", \"restaurant\", \"place_name\", \"name\"], required=False)\n",
    "        col_author   = _find_col(df, [\"author_name\", \"user\", \"user_name\", \"reviewer\"], required=False)\n",
    "        col_text     = _find_col(df, [\"text\", \"review_text\", \"comment\", \"content\", \"review\"], required=True)\n",
    "        col_rating   = _find_col(df, [\"rating\", \"stars\", \"score\", \"star_rating\"], required=False)\n",
    "\n",
    "        # Optional columns may or may not exist\n",
    "        col_photo          = _find_col(df, [\"photo\"], required=False)\n",
    "        col_rating_category= _find_col(df, [\"rating_category\"], required=False)\n",
    "\n",
    "        # Work on a copy\n",
    "        d = df.copy()\n",
    "\n",
    "        # Normalize whitespace for string fields (only if they exist)\n",
    "        if col_text:\n",
    "            d[col_text] = d[col_text].astype(str).str.strip()\n",
    "        if col_business:\n",
    "            d[col_business] = d[col_business].astype(str).str.strip()\n",
    "        if col_author:\n",
    "            d[col_author] = d[col_author].astype(str).str.strip()\n",
    "\n",
    "        # Coerce rating to numeric if exists\n",
    "        if col_rating:\n",
    "            d[col_rating] = pd.to_numeric(d[col_rating], errors=\"coerce\")\n",
    "            d[col_rating] = d[col_rating].fillna(3)  # Default to 3 if NaN\n",
    "            d[col_rating] = d[col_rating].clip(1, 5)  # Ensure 1-5 range\n",
    "\n",
    "        # Drop rows with missing/empty required fields\n",
    "        before = len(d)\n",
    "        d = d.dropna(subset=[col_text])\n",
    "        # Remove empty-string rows in text column\n",
    "        d = d[d[col_text] != \"\"]\n",
    "        # Remove very short reviews\n",
    "        d = d[d[col_text].str.len() >= 5]\n",
    "\n",
    "        removed = before - len(d)\n",
    "        print(f\"🧹 Cleaned dataset: {before} → {len(d)} rows (removed {removed})\")\n",
    "\n",
    "        if len(d) == 0:\n",
    "            print(\"⚠️ No valid reviews remaining after cleaning\")\n",
    "            return None\n",
    "\n",
    "        # Rebuild output with target column names in the same format as Kaggle\n",
    "        out_data = {\n",
    "            \"text\": d[col_text],\n",
    "            \"rating\": d[col_rating] if col_rating else pd.Series([3]*len(d))\n",
    "        }\n",
    "        \n",
    "        # Add optional columns if they exist\n",
    "        if col_business:\n",
    "            out_data[\"business_name\"] = d[col_business]\n",
    "        else:\n",
    "            out_data[\"business_name\"] = pd.Series(['Unknown Business']*len(d))\n",
    "            \n",
    "        if col_author:\n",
    "            out_data[\"author_name\"] = d[col_author]\n",
    "        else:\n",
    "            out_data[\"author_name\"] = pd.Series([f'user_{i}' for i in range(len(d))])\n",
    "\n",
    "        out = pd.DataFrame(out_data)\n",
    "        \n",
    "        # Attach optional columns if present\n",
    "        if col_photo and col_photo in d.columns:\n",
    "            out[\"photo\"] = d[col_photo]\n",
    "        else:\n",
    "            out[\"photo\"] = pd.Series([pd.NA]*len(d))\n",
    "            \n",
    "        if col_rating_category and col_rating_category in d.columns:\n",
    "            out[\"rating_category\"] = d[col_rating_category]\n",
    "        else:\n",
    "            out[\"rating_category\"] = pd.Series([pd.NA]*len(d))\n",
    "\n",
    "        return out.reset_index(drop=True)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error during cleaning: {e}\")\n",
    "        return None\n",
    "\n",
    "def merge_all_datasets_and_sample(kaggle_df, additional_dfs, sample_size=100):\n",
    "    \"\"\"Merge all datasets and sample specified number of reviews\"\"\"\n",
    "    \n",
    "    if not additional_dfs:\n",
    "        print(\"ℹ️ No additional datasets found, using Kaggle data only\")\n",
    "        combined_df = kaggle_df.copy()\n",
    "    else:\n",
    "        print(f\"🔄 Merging {len(additional_dfs)} additional datasets with Kaggle data...\")\n",
    "        \n",
    "        # Combine all datasets\n",
    "        all_dfs = [kaggle_df]\n",
    "        \n",
    "        for i, add_df in enumerate(additional_dfs):\n",
    "            print(f\"   Adding dataset {i+1}/{len(additional_dfs)} with {len(add_df)} rows\")\n",
    "            all_dfs.append(add_df)\n",
    "        \n",
    "        try:\n",
    "            combined_df = pd.concat(all_dfs, ignore_index=True)\n",
    "            \n",
    "            # Remove duplicates based on text content\n",
    "            original_len = len(combined_df)\n",
    "            combined_df = combined_df.drop_duplicates(subset=['text'], keep='first')\n",
    "            duplicates_removed = original_len - len(combined_df)\n",
    "            \n",
    "            print(f\"📊 Merged dataset statistics:\")\n",
    "            print(f\"   Total reviews: {len(combined_df)}\")\n",
    "            print(f\"   Kaggle reviews: {len(kaggle_df)}\")\n",
    "            print(f\"   Additional reviews: {len(combined_df) - len(kaggle_df)}\")\n",
    "            print(f\"   Duplicates removed: {duplicates_removed}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error merging datasets: {e}\")\n",
    "            print(\"🔄 Falling back to Kaggle data only\")\n",
    "            combined_df = kaggle_df.copy()\n",
    "    \n",
    "    # Sample the specified number of reviews\n",
    "    if len(combined_df) > sample_size:\n",
    "        print(f\"🎯 Sampling {sample_size} reviews from {len(combined_df)} total reviews...\")\n",
    "        sampled_df = combined_df.sample(n=sample_size, random_state=42).reset_index(drop=True)\n",
    "    else:\n",
    "        print(f\"🔄 Using all {len(combined_df)} reviews (less than requested {sample_size})\")\n",
    "        sampled_df = combined_df.reset_index(drop=True)\n",
    "    \n",
    "    print(f\"📊 Final sample size: {len(sampled_df)} reviews\")\n",
    "    print(f\"📋 Final columns: {list(sampled_df.columns)}\")\n",
    "    \n",
    "    return sampled_df\n",
    "\n",
    "# Test the JSONL parsing specifically\n",
    "print(\"🧪 Testing JSONL parsing on your review-other.json file...\")\n",
    "test_path = './review-other.json'\n",
    "\n",
    "if os.path.exists(test_path):\n",
    "    try:\n",
    "        # Load and show sample data\n",
    "        sample_objects = []\n",
    "        with open(test_path, 'r', encoding='utf-8') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                if i >= 5:  # Just get first 5 lines for testing\n",
    "                    break\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    try:\n",
    "                        obj = json.loads(line)\n",
    "                        sample_objects.append(obj)\n",
    "                    except json.JSONDecodeError as e:\n",
    "                        print(f\"❌ Error parsing line {i+1}: {e}\")\n",
    "        \n",
    "        if sample_objects:\n",
    "            sample_df = pd.DataFrame(sample_objects)\n",
    "            print(f\"✅ Successfully parsed sample data:\")\n",
    "            print(f\"📊 Shape: {sample_df.shape}\")\n",
    "            print(f\"📋 Original columns: {list(sample_df.columns)}\")\n",
    "            \n",
    "            # Test cleaning\n",
    "            cleaned_sample = clean_json_data_like_kaggle(sample_df)\n",
    "            if cleaned_sample is not None:\n",
    "                print(f\"✅ Cleaned sample:\")\n",
    "                print(f\"📊 Shape: {cleaned_sample.shape}\")\n",
    "                print(f\"📋 Columns: {list(cleaned_sample.columns)}\")\n",
    "                print(f\"🔍 First few rows:\")\n",
    "                print(cleaned_sample.head(3))\n",
    "        else:\n",
    "            print(\"❌ No valid JSON objects found in sample\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error testing JSONL parsing: {e}\")\n",
    "else:\n",
    "    print(f\"❌ File not found: {test_path}\")\n",
    "\n",
    "# Check if we have a loaded dataframe\n",
    "if 'df' in locals() and not df.empty:\n",
    "    \n",
    "    # Load additional review datasets\n",
    "    print(\"📂 Searching for additional review datasets...\")\n",
    "    additional_datasets = load_additional_reviews()\n",
    "    \n",
    "    # Merge all datasets and sample 100 reviews\n",
    "    sample_df = merge_all_datasets_and_sample(df, additional_datasets, sample_size=100)\n",
    "    \n",
    "    # Verify we have the required columns\n",
    "    if 'text' not in sample_df.columns:\n",
    "        print(\"❌ No text column found in sample dataset\")\n",
    "        print(\"🔄 Please check your data sources\")\n",
    "    else:\n",
    "        print(f\"📝 Using 'text' column for review text\")\n",
    "        \n",
    "        # Display sample dataset information\n",
    "        print(f\"\\n📊 SAMPLE DATASET INFORMATION:\")\n",
    "        print(f\"   Total reviews for processing: {len(sample_df)}\")\n",
    "        print(f\"   Columns: {list(sample_df.columns)}\")\n",
    "        \n",
    "        if 'rating' in sample_df.columns:\n",
    "            rating_dist = dict(sample_df['rating'].value_counts().sort_index())\n",
    "            print(f\"   Rating distribution: {rating_dist}\")\n",
    "        \n",
    "        # Show a sample of the data\n",
    "        print(f\"\\n🔍 Sample of data:\")\n",
    "        for i, row in sample_df.head(5).iterrows():\n",
    "            text = str(row['text'])[:100]\n",
    "            rating = row.get('rating', 'N/A')\n",
    "            print(f\"   {i+1}. Rating: {rating} | Text: {text}{'...' if len(str(row['text'])) > 100 else ''}\")\n",
    "        print()\n",
    "        \n",
    "        # Process reviews and generate labels\n",
    "        if gemma_classifier and gemma_classifier.client:\n",
    "            print(f\"🔄 Processing {len(sample_df)} reviews with Gemma classifier...\")\n",
    "            print(f\"🚨 RESULTS (Text + Labels):\")\n",
    "            print(\"-\" * 60)\n",
    "            \n",
    "            results_for_output = []\n",
    "            \n",
    "            for idx, row in sample_df.iterrows():\n",
    "                review_text = str(row['text']) if pd.notna(row['text']) else \"\"\n",
    "                review_rating = row.get('rating', 3)\n",
    "                \n",
    "                # Progress indicator\n",
    "                if (idx + 1) % 10 == 0:\n",
    "                    print(f\"\\n📊 Progress: {idx + 1}/{len(sample_df)} reviews\")\n",
    "                    print(\"-\" * 60)\n",
    "                \n",
    "                # Skip empty reviews\n",
    "                if len(review_text.strip()) < 5:\n",
    "                    results_for_output.append({\n",
    "                        'index': idx + 1,\n",
    "                        'text': review_text,\n",
    "                        'rating': review_rating,\n",
    "                        'label': 'INVALID',\n",
    "                        'reason': 'Review too short'\n",
    "                    })\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    # Classify with Gemma\n",
    "                    result = gemma_classifier.classify_review(review_text)\n",
    "                    \n",
    "                    # Determine label\n",
    "                    is_violation = result.get('is_violation', False)\n",
    "                    violation_type = result.get('violation_type', 'unknown')\n",
    "                    confidence = result.get('policy_confidence', 0.0)\n",
    "                    \n",
    "                    if is_violation:\n",
    "                        label = f\"VIOLATION ({violation_type.upper()})\"\n",
    "                    else:\n",
    "                        label = \"LEGITIMATE\"\n",
    "                    \n",
    "                    results_for_output.append({\n",
    "                        'index': idx + 1,\n",
    "                        'text': review_text,\n",
    "                        'rating': review_rating,\n",
    "                        'label': label,\n",
    "                        'confidence': confidence,\n",
    "                        'reason': result.get('policy_reasoning', '')\n",
    "                    })\n",
    "                    \n",
    "                    # Display result\n",
    "                    print(f\"\\n📝 Review #{idx + 1}: {label}\")\n",
    "                    print(f\"⭐ Rating: {review_rating}\")\n",
    "                    print(f\"🎯 Confidence: {confidence:.3f}\")\n",
    "                    print(f\"📄 Text: \\\"{review_text[:150]}{'...' if len(review_text) > 150 else ''}\\\"\")\n",
    "                    if result.get('policy_reasoning'):\n",
    "                        print(f\"💡 Reason: {result.get('policy_reasoning', '')[:100]}{'...' if len(result.get('policy_reasoning', '')) > 100 else ''}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    results_for_output.append({\n",
    "                        'index': idx + 1,\n",
    "                        'text': review_text,\n",
    "                        'rating': review_rating,\n",
    "                        'label': 'ERROR',\n",
    "                        'reason': str(e)\n",
    "                    })\n",
    "                    print(f\"\\n❌ ERROR processing review #{idx + 1}: {str(e)}\")\n",
    "                \n",
    "                # Rate limiting\n",
    "                time.sleep(0.8)\n",
    "            \n",
    "            # Final summary and save results\n",
    "            print(f\"\\n\" + \"=\"*70)\n",
    "            print(f\"📊 PROCESSING COMPLETE!\")\n",
    "            \n",
    "            # Count labels\n",
    "            label_counts = {}\n",
    "            for result in results_for_output:\n",
    "                label = result['label'].split(' (')[0]  # Get main label part\n",
    "                label_counts[label] = label_counts.get(label, 0) + 1\n",
    "            \n",
    "            print(f\"📋 Label distribution:\")\n",
    "            for label, count in label_counts.items():\n",
    "                print(f\"   {label}: {count}\")\n",
    "            \n",
    "            # Save results to CSV\n",
    "            results_df = pd.DataFrame(results_for_output)\n",
    "            output_filename = f\"sample_100_reviews_with_labels_{int(time.time())}.csv\"\n",
    "            results_df.to_csv(output_filename, index=False)\n",
    "            print(f\"💾 Results saved to: {output_filename}\")\n",
    "            print(f\"📊 Output columns: {list(results_df.columns)}\")\n",
    "            \n",
    "            # Show first few results\n",
    "            print(f\"\\n🔍 First 5 results:\")\n",
    "            print(results_df[['index', 'label', 'rating']].head())\n",
    "            \n",
    "        else:\n",
    "            print(\"⚠️ Gemma classifier not available\")\n",
    "            print(\"💡 Generating simple output without ML classification...\")\n",
    "            \n",
    "            # Simple output without classification\n",
    "            simple_results = []\n",
    "            for idx, row in sample_df.iterrows():\n",
    "                simple_results.append({\n",
    "                    'index': idx + 1,\n",
    "                    'text': str(row['text']),\n",
    "                    'rating': row.get('rating', 'N/A'),\n",
    "                    'label': 'NOT_CLASSIFIED',\n",
    "                    'reason': 'Classifier not available'\n",
    "                })\n",
    "            \n",
    "            # Save simple results\n",
    "            simple_df = pd.DataFrame(simple_results)\n",
    "            output_filename = f\"sample_100_reviews_no_classification_{int(time.time())}.csv\"\n",
    "            simple_df.to_csv(output_filename, index=False)\n",
    "            print(f\"💾 Simple results saved to: {output_filename}\")\n",
    "\n",
    "elif 'df' not in locals() or df.empty:\n",
    "    print(\"⚠️ No dataset loaded yet\")\n",
    "    print(\"💡 Please run the data loading cells first to load your review dataset\")\n",
    "\n",
    "else:\n",
    "    print(\"❌ Dataset not available\")\n",
    "    print(\"🔄 Please ensure dataset is properly loaded\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e14e57f",
   "metadata": {},
   "source": [
    "## 🎯 Traditional ML Models\n",
    "\n",
    "Building and training traditional machine learning models for review classification and policy detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf21359",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "class TraditionalMLPipeline:\n",
    "    \"\"\"Traditional ML pipeline for review classification\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.models = {}\n",
    "        self.feature_extractor = None\n",
    "        self.scaler = StandardScaler()\n",
    "        self.text_vectorizer = TfidfVectorizer(\n",
    "            max_features=5000,\n",
    "            ngram_range=(1, 2),\n",
    "            stop_words='english',\n",
    "            lowercase=True,\n",
    "            min_df=2,\n",
    "            max_df=0.95\n",
    "        )\n",
    "        self.is_fitted = False\n",
    "        \n",
    "        # Initialize models\n",
    "        self._init_models()\n",
    "    \n",
    "    def _init_models(self):\n",
    "        \"\"\"Initialize ML models\"\"\"\n",
    "        \n",
    "        self.models = {\n",
    "            'logistic_regression': LogisticRegression(\n",
    "                random_state=42,\n",
    "                max_iter=1000,\n",
    "                class_weight='balanced'\n",
    "            ),\n",
    "            'random_forest': RandomForestClassifier(\n",
    "                n_estimators=100,\n",
    "                random_state=42,\n",
    "                class_weight='balanced',\n",
    "                max_depth=10,\n",
    "                min_samples_split=5\n",
    "            ),\n",
    "            'gradient_boosting': GradientBoostingClassifier(\n",
    "                n_estimators=100,\n",
    "                random_state=42,\n",
    "                learning_rate=0.1,\n",
    "                max_depth=6\n",
    "            ),\n",
    "            'svm': SVC(\n",
    "                random_state=42,\n",
    "                probability=True,\n",
    "                class_weight='balanced',\n",
    "                kernel='rbf'\n",
    "            ),\n",
    "            'naive_bayes': MultinomialNB(alpha=1.0)\n",
    "        }\n",
    "    \n",
    "    def prepare_features(self, df, text_column='text', target_column='is_violation'):\n",
    "        \"\"\"Prepare features for ML models\"\"\"\n",
    "        \n",
    "        print(\"🔄 Preparing features for ML models...\")\n",
    "        \n",
    "        # Extract advanced features if feature extractor exists\n",
    "        if hasattr(self, 'feature_extractor') and self.feature_extractor:\n",
    "            print(\"📊 Extracting advanced features...\")\n",
    "            feature_df = self.feature_extractor.extract_features(df, text_column)\n",
    "        else:\n",
    "            print(\"⚠️ No feature extractor found, using basic features...\")\n",
    "            feature_df = df.copy()\n",
    "        \n",
    "        # Text vectorization\n",
    "        print(\"📝 Vectorizing text...\")\n",
    "        text_features = self.text_vectorizer.fit_transform(df[text_column])\n",
    "        \n",
    "        # Combine features\n",
    "        numerical_features = []\n",
    "        feature_names = []\n",
    "        \n",
    "        # Select numerical features (excluding target and text)\n",
    "        for col in feature_df.columns:\n",
    "            if col not in [target_column, text_column] and pd.api.types.is_numeric_dtype(feature_df[col]):\n",
    "                numerical_features.append(feature_df[col].fillna(0))\n",
    "                feature_names.append(col)\n",
    "        \n",
    "        if numerical_features:\n",
    "            numerical_array = np.column_stack(numerical_features)\n",
    "            numerical_array = self.scaler.fit_transform(numerical_array)\n",
    "            \n",
    "            # Combine text and numerical features\n",
    "            combined_features = np.hstack([text_features.toarray(), numerical_array])\n",
    "        else:\n",
    "            combined_features = text_features.toarray()\n",
    "        \n",
    "        # Feature names for interpretability\n",
    "        text_feature_names = [f\"text_{i}\" for i in range(text_features.shape[1])]\n",
    "        all_feature_names = text_feature_names + feature_names\n",
    "        \n",
    "        print(f\"✅ Prepared {combined_features.shape[1]} features from {len(df)} samples\")\n",
    "        \n",
    "        return combined_features, all_feature_names\n",
    "    \n",
    "    def train_models(self, df, text_column='text', target_column='is_violation', test_size=0.2):\n",
    "        \"\"\"Train all ML models\"\"\"\n",
    "        \n",
    "        print(\"🚀 Starting ML model training...\")\n",
    "        \n",
    "        # Prepare features\n",
    "        X, feature_names = self.prepare_features(df, text_column, target_column)\n",
    "        y = df[target_column].astype(int)\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=test_size, random_state=42, stratify=y\n",
    "        )\n",
    "        \n",
    "        print(f\"📊 Training set: {len(X_train)} samples\")\n",
    "        print(f\"📊 Test set: {len(X_test)} samples\")\n",
    "        print(f\"📊 Positive samples: {sum(y_train)} ({sum(y_train)/len(y_train)*100:.1f}%)\") \n",
    "        \n",
    "        # Train models\n",
    "        results = {}\n",
    "        \n",
    "        for name, model in self.models.items():\n",
    "            print(f\"\\n🔄 Training {name}...\")\n",
    "            \n",
    "            try:\n",
    "                # Train model\n",
    "                start_time = time.time()\n",
    "                model.fit(X_train, y_train)\n",
    "                train_time = time.time() - start_time\n",
    "                \n",
    "                # Predictions\n",
    "                y_pred = model.predict(X_test)\n",
    "                y_prob = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "                \n",
    "                # Metrics\n",
    "                accuracy = model.score(X_test, y_test)\n",
    "                \n",
    "                if y_prob is not None:\n",
    "                    auc_score = roc_auc_score(y_test, y_prob)\n",
    "                else:\n",
    "                    auc_score = None\n",
    "                \n",
    "                # Cross-validation\n",
    "                cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')\n",
    "                \n",
    "                results[name] = {\n",
    "                    'model': model,\n",
    "                    'accuracy': accuracy,\n",
    "                    'auc_score': auc_score,\n",
    "                    'cv_mean': cv_scores.mean(),\n",
    "                    'cv_std': cv_scores.std(),\n",
    "                    'train_time': train_time,\n",
    "                    'predictions': y_pred,\n",
    "                    'probabilities': y_prob,\n",
    "                    'y_test': y_test\n",
    "                }\n",
    "                \n",
    "                print(f\"✅ {name}: Accuracy = {accuracy:.3f}, AUC = {auc_score:.3f if auc_score else 'N/A'}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error training {name}: {e}\")\n",
    "                results[name] = {'error': str(e)}\n",
    "        \n",
    "        # Store results\n",
    "        self.training_results = results\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        self.feature_names = feature_names\n",
    "        self.is_fitted = True\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def create_ensemble(self):\n",
    "        \"\"\"Create ensemble model from trained models\"\"\"\n",
    "        \n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Models must be trained first\")\n",
    "        \n",
    "        # Select best performing models\n",
    "        valid_models = []\n",
    "        for name, result in self.training_results.items():\n",
    "            if 'model' in result and 'error' not in result:\n",
    "                if result['accuracy'] > 0.6:  # Minimum threshold\n",
    "                    valid_models.append((name, result['model']))\n",
    "        \n",
    "        if len(valid_models) < 2:\n",
    "            print(\"⚠️ Not enough valid models for ensemble\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"🎯 Creating ensemble from {len(valid_models)} models: {[name for name, _ in valid_models]}\")\n",
    "        \n",
    "        # Create voting classifier\n",
    "        ensemble = VotingClassifier(\n",
    "            estimators=valid_models,\n",
    "            voting='soft' if all(hasattr(model, 'predict_proba') for _, model in valid_models) else 'hard'\n",
    "        )\n",
    "        \n",
    "        # This ensemble is already fitted since constituent models are fitted\n",
    "        self.ensemble_model = ensemble\n",
    "        \n",
    "        return ensemble\n",
    "    \n",
    "    def evaluate_models(self, show_plots=True):\n",
    "        \"\"\"Comprehensive model evaluation\"\"\"\n",
    "        \n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Models must be trained first\")\n",
    "        \n",
    "        print(\"📊 Model Evaluation Results\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Performance summary\n",
    "        performance_data = []\n",
    "        \n",
    "        for name, result in self.training_results.items():\n",
    "            if 'model' in result and 'error' not in result:\n",
    "                performance_data.append({\n",
    "                    'Model': name.replace('_', ' ').title(),\n",
    "                    'Accuracy': result['accuracy'],\n",
    "                    'AUC Score': result['auc_score'] if result['auc_score'] else 0,\n",
    "                    'CV Mean': result['cv_mean'],\n",
    "                    'CV Std': result['cv_std'],\n",
    "                    'Train Time (s)': result['train_time']\n",
    "                })\n",
    "        \n",
    "        performance_df = pd.DataFrame(performance_data)\n",
    "        print(\"\\n📈 Performance Summary:\")\n",
    "        print(performance_df.round(3).to_string(index=False))\n",
    "        \n",
    "        if show_plots:\n",
    "            self._plot_model_comparison(performance_df)\n",
    "            self._plot_roc_curves()\n",
    "        \n",
    "        return performance_df\n",
    "    \n",
    "    def _plot_model_comparison(self, performance_df):\n",
    "        \"\"\"Plot model comparison charts\"\"\"\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        fig.suptitle('Model Performance Comparison', fontsize=16)\n",
    "        \n",
    "        # Accuracy comparison\n",
    "        axes[0,0].bar(performance_df['Model'], performance_df['Accuracy'], color='skyblue')\n",
    "        axes[0,0].set_title('Accuracy Comparison')\n",
    "        axes[0,0].set_ylabel('Accuracy')\n",
    "        axes[0,0].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # AUC comparison\n",
    "        axes[0,1].bar(performance_df['Model'], performance_df['AUC Score'], color='lightgreen')\n",
    "        axes[0,1].set_title('AUC Score Comparison')\n",
    "        axes[0,1].set_ylabel('AUC Score')\n",
    "        axes[0,1].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # CV scores with error bars\n",
    "        axes[1,0].bar(performance_df['Model'], performance_df['CV Mean'], \n",
    "                     yerr=performance_df['CV Std'], color='orange', capsize=5)\n",
    "        axes[1,0].set_title('Cross-Validation Scores')\n",
    "        axes[1,0].set_ylabel('CV Accuracy')\n",
    "        axes[1,0].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Training time\n",
    "        axes[1,1].bar(performance_df['Model'], performance_df['Train Time (s)'], color='coral')\n",
    "        axes[1,1].set_title('Training Time')\n",
    "        axes[1,1].set_ylabel('Time (seconds)')\n",
    "        axes[1,1].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def _plot_roc_curves(self):\n",
    "        \"\"\"Plot ROC curves for models with probability outputs\"\"\"\n",
    "        \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        \n",
    "        for name, result in self.training_results.items():\n",
    "            if 'probabilities' in result and result['probabilities'] is not None:\n",
    "                fpr, tpr, _ = roc_curve(result['y_test'], result['probabilities'])\n",
    "                auc_score = result['auc_score']\n",
    "                plt.plot(fpr, tpr, label=f\"{name.replace('_', ' ').title()} (AUC = {auc_score:.3f})\")\n",
    "        \n",
    "        plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('ROC Curves Comparison')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "    \n",
    "    def predict_review(self, review_text: str, use_ensemble: bool = True):\n",
    "        \"\"\"Predict policy violation for a single review\"\"\"\n",
    "        \n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Models must be trained first\")\n",
    "        \n",
    "        # Create temporary dataframe\n",
    "        temp_df = pd.DataFrame({'text': [review_text]})\n",
    "        \n",
    "        # Prepare features\n",
    "        X, _ = self.prepare_features(temp_df, 'text', None)\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        if use_ensemble and hasattr(self, 'ensemble_model'):\n",
    "            # Use ensemble\n",
    "            pred = self.ensemble_model.predict(X)[0]\n",
    "            prob = self.ensemble_model.predict_proba(X)[0] if hasattr(self.ensemble_model, 'predict_proba') else None\n",
    "            \n",
    "            results['ensemble'] = {\n",
    "                'prediction': bool(pred),\n",
    "                'probability': prob[1] if prob is not None else None\n",
    "            }\n",
    "        \n",
    "        # Individual model predictions\n",
    "        for name, result in self.training_results.items():\n",
    "            if 'model' in result and 'error' not in result:\n",
    "                model = result['model']\n",
    "                pred = model.predict(X)[0]\n",
    "                prob = model.predict_proba(X)[0] if hasattr(model, 'predict_proba') else None\n",
    "                \n",
    "                results[name] = {\n",
    "                    'prediction': bool(pred),\n",
    "                    'probability': prob[1] if prob is not None else None\n",
    "                }\n",
    "        \n",
    "        return results\n",
    "\n",
    "print(\"🎯 Traditional ML Pipeline initialized successfully!\")\n",
    "print(\"📊 Available models: Logistic Regression, Random Forest, Gradient Boosting, SVM, Naive Bayes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2925844",
   "metadata": {},
   "source": [
    "## 🔀 Ensemble & Hybrid Approach\n",
    "\n",
    "Combining rule-based detection, traditional ML, and Gemma 3 12B for comprehensive review analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca9ce34",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridReviewClassifier:\n",
    "    \"\"\"Hybrid classifier combining rule-based, traditional ML, and LLM approaches\"\"\"\n",
    "    \n",
    "    def __init__(self, policy_detector, ml_pipeline, gemma_classifier=None):\n",
    "        self.policy_detector = policy_detector\n",
    "        self.ml_pipeline = ml_pipeline\n",
    "        self.gemma_classifier = gemma_classifier\n",
    "        self.weights = {\n",
    "            'rule_based': 0.3,\n",
    "            'traditional_ml': 0.4,\n",
    "            'llm': 0.3\n",
    "        }\n",
    "    \n",
    "    def set_weights(self, rule_based=0.3, traditional_ml=0.4, llm=0.3):\n",
    "        \"\"\"Set weights for different approaches\"\"\"\n",
    "        total = rule_based + traditional_ml + llm\n",
    "        self.weights = {\n",
    "            'rule_based': rule_based / total,\n",
    "            'traditional_ml': traditional_ml / total,\n",
    "            'llm': llm / total\n",
    "        }\n",
    "    \n",
    "    def classify_review(self, review_text: str, use_llm: bool = True) -> Dict:\n",
    "        \"\"\"Comprehensive review classification using all approaches\"\"\"\n",
    "        \n",
    "        if not review_text or len(review_text.strip()) < 5:\n",
    "            return {\n",
    "                'error': 'Review text too short',\n",
    "                'is_violation': False,\n",
    "                'confidence': 0.0\n",
    "            }\n",
    "        \n",
    "        results = {\n",
    "            'review_text': review_text[:100] + '...' if len(review_text) > 100 else review_text,\n",
    "            'timestamp': time.time()\n",
    "        }\n",
    "        \n",
    "        # 1. Rule-based detection\n",
    "        try:\n",
    "            rule_result = self.policy_detector.analyze_review(review_text)\n",
    "            results['rule_based'] = {\n",
    "                'is_violation': rule_result['is_violation'],\n",
    "                'violation_type': rule_result['violation_type'],\n",
    "                'confidence': rule_result['confidence'],\n",
    "                'all_scores': rule_result['all_scores']\n",
    "            }\n",
    "        except Exception as e:\n",
    "            results['rule_based'] = {'error': str(e)}\n",
    "        \n",
    "        # 2. Traditional ML prediction\n",
    "        try:\n",
    "            if self.ml_pipeline.is_fitted:\n",
    "                ml_predictions = self.ml_pipeline.predict_review(review_text, use_ensemble=True)\n",
    "                \n",
    "                # Use ensemble if available, otherwise best performing model\n",
    "                if 'ensemble' in ml_predictions:\n",
    "                    ml_result = ml_predictions['ensemble']\n",
    "                else:\n",
    "                    # Find best model based on training results\n",
    "                    best_model = max(\n",
    "                        [k for k in ml_predictions.keys() if 'error' not in ml_predictions[k]],\n",
    "                        key=lambda x: self.ml_pipeline.training_results[x].get('accuracy', 0),\n",
    "                        default=list(ml_predictions.keys())[0]\n",
    "                    )\n",
    "                    ml_result = ml_predictions[best_model]\n",
    "                \n",
    "                results['traditional_ml'] = {\n",
    "                    'is_violation': ml_result['prediction'],\n",
    "                    'confidence': ml_result['probability'] if ml_result['probability'] else 0.5,\n",
    "                    'all_predictions': ml_predictions\n",
    "                }\n",
    "            else:\n",
    "                results['traditional_ml'] = {'error': 'Models not trained'}\n",
    "        except Exception as e:\n",
    "            results['traditional_ml'] = {'error': str(e)}\n",
    "        \n",
    "        # 3. LLM classification\n",
    "        if use_llm and self.gemma_classifier and self.gemma_classifier.client:\n",
    "            try:\n",
    "                llm_result = self.gemma_classifier.classify_review(review_text)\n",
    "                results['llm'] = {\n",
    "                    'is_violation': llm_result.get('is_violation', False),\n",
    "                    'violation_type': llm_result.get('violation_type'),\n",
    "                    'confidence': llm_result.get('policy_confidence', 0.0),\n",
    "                    'quality_score': llm_result.get('quality_score', 0.5),\n",
    "                    'is_trustworthy': llm_result.get('is_trustworthy', True),\n",
    "                    'recommendation': llm_result.get('recommendation', 'keep')\n",
    "                }\n",
    "            except Exception as e:\n",
    "                results['llm'] = {'error': str(e)}\n",
    "        else:\n",
    "            results['llm'] = {'error': 'LLM not available or disabled'}\n",
    "        \n",
    "        # 4. Ensemble decision\n",
    "        final_result = self._compute_ensemble_decision(results)\n",
    "        results.update(final_result)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _compute_ensemble_decision(self, results: Dict) -> Dict:\n",
    "        \"\"\"Compute final ensemble decision from all approaches\"\"\"\n",
    "        \n",
    "        violation_scores = []\n",
    "        confidence_scores = []\n",
    "        violation_types = []\n",
    "        \n",
    "        # Rule-based contribution\n",
    "        if 'rule_based' in results and 'error' not in results['rule_based']:\n",
    "            rule_data = results['rule_based']\n",
    "            violation_score = rule_data['confidence'] if rule_data['is_violation'] else 0\n",
    "            violation_scores.append(violation_score * self.weights['rule_based'])\n",
    "            confidence_scores.append(rule_data['confidence'])\n",
    "            if rule_data['violation_type']:\n",
    "                violation_types.append(rule_data['violation_type'])\n",
    "        \n",
    "        # Traditional ML contribution\n",
    "        if 'traditional_ml' in results and 'error' not in results['traditional_ml']:\n",
    "            ml_data = results['traditional_ml']\n",
    "            violation_score = ml_data['confidence'] if ml_data['is_violation'] else (1 - ml_data['confidence'])\n",
    "            violation_scores.append(violation_score * self.weights['traditional_ml'])\n",
    "            confidence_scores.append(ml_data['confidence'])\n",
    "        \n",
    "        # LLM contribution\n",
    "        if 'llm' in results and 'error' not in results['llm']:\n",
    "            llm_data = results['llm']\n",
    "            violation_score = llm_data['confidence'] if llm_data['is_violation'] else 0\n",
    "            violation_scores.append(violation_score * self.weights['llm'])\n",
    "            confidence_scores.append(llm_data['confidence'])\n",
    "            if llm_data['violation_type']:\n",
    "                violation_types.append(llm_data['violation_type'])\n",
    "        \n",
    "        # Compute final decision\n",
    "        if not violation_scores:\n",
    "            return {\n",
    "                'final_decision': {\n",
    "                    'is_violation': False,\n",
    "                    'confidence': 0.0,\n",
    "                    'violation_type': None,\n",
    "                    'reasoning': 'No valid predictions available'\n",
    "                }\n",
    "            }\n",
    "        \n",
    "        # Weighted average\n",
    "        final_violation_score = sum(violation_scores)\n",
    "        avg_confidence = np.mean(confidence_scores)\n",
    "        \n",
    "        # Determine violation type (most common)\n",
    "        if violation_types:\n",
    "            violation_type = max(set(violation_types), key=violation_types.count)\n",
    "        else:\n",
    "            violation_type = None\n",
    "        \n",
    "        # Decision threshold\n",
    "        is_violation = final_violation_score > 0.5\n",
    "        \n",
    "        # Compute reasoning\n",
    "        active_methods = []\n",
    "        if 'rule_based' in results and 'error' not in results['rule_based']:\n",
    "            active_methods.append(f\"Rule-based: {results['rule_based']['confidence']:.2f}\")\n",
    "        if 'traditional_ml' in results and 'error' not in results['traditional_ml']:\n",
    "            active_methods.append(f\"ML: {results['traditional_ml']['confidence']:.2f}\")\n",
    "        if 'llm' in results and 'error' not in results['llm']:\n",
    "            active_methods.append(f\"LLM: {results['llm']['confidence']:.2f}\")\n",
    "        \n",
    "        reasoning = f\"Ensemble decision from {len(active_methods)} methods: {', '.join(active_methods)}\"\n",
    "        \n",
    "        return {\n",
    "            'final_decision': {\n",
    "                'is_violation': is_violation,\n",
    "                'confidence': final_violation_score,\n",
    "                'avg_confidence': avg_confidence,\n",
    "                'violation_type': violation_type if is_violation else None,\n",
    "                'reasoning': reasoning,\n",
    "                'methods_used': len(active_methods),\n",
    "                'weighted_score': final_violation_score\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def batch_classify(self, reviews: List[str], use_llm: bool = True, batch_size: int = 10) -> List[Dict]:\n",
    "        \"\"\"Classify multiple reviews\"\"\"\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        for i in range(0, len(reviews), batch_size):\n",
    "            batch = reviews[i:i + batch_size]\n",
    "            print(f\"🔄 Processing batch {i//batch_size + 1}/{(len(reviews)-1)//batch_size + 1}\")\n",
    "            \n",
    "            batch_results = []\n",
    "            for j, review in enumerate(batch):\n",
    "                print(f\"   Processing review {i + j + 1}/{len(reviews)}\", end='\\\\r')\n",
    "                result = self.classify_review(review, use_llm=use_llm)\n",
    "                batch_results.append(result)\n",
    "            \n",
    "            results.extend(batch_results)\n",
    "        \n",
    "        print(f\"\\\\n✅ Completed classification of {len(reviews)} reviews\")\n",
    "        return results\n",
    "    \n",
    "    def analyze_performance(self, results: List[Dict]) -> Dict:\n",
    "        \"\"\"Analyze performance of the hybrid classifier\"\"\"\n",
    "        \n",
    "        analysis = {\n",
    "            'total_reviews': len(results),\n",
    "            'violations_detected': 0,\n",
    "            'method_availability': {\n",
    "                'rule_based': 0,\n",
    "                'traditional_ml': 0,\n",
    "                'llm': 0\n",
    "            },\n",
    "            'violation_types': {},\n",
    "            'confidence_distribution': [],\n",
    "            'agreement_analysis': {\n",
    "                'all_agree': 0,\n",
    "                'majority_agree': 0,\n",
    "                'no_agreement': 0\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        for result in results:\n",
    "            # Count violations\n",
    "            if result.get('final_decision', {}).get('is_violation', False):\n",
    "                analysis['violations_detected'] += 1\n",
    "                \n",
    "                violation_type = result['final_decision'].get('violation_type')\n",
    "                if violation_type:\n",
    "                    analysis['violation_types'][violation_type] = \\\n",
    "                        analysis['violation_types'].get(violation_type, 0) + 1\n",
    "            \n",
    "            # Method availability\n",
    "            for method in ['rule_based', 'traditional_ml', 'llm']:\n",
    "                if method in result and 'error' not in result[method]:\n",
    "                    analysis['method_availability'][method] += 1\n",
    "            \n",
    "            # Confidence distribution\n",
    "            confidence = result.get('final_decision', {}).get('confidence', 0)\n",
    "            analysis['confidence_distribution'].append(confidence)\n",
    "            \n",
    "            # Agreement analysis\n",
    "            predictions = []\n",
    "            for method in ['rule_based', 'traditional_ml', 'llm']:\n",
    "                if method in result and 'error' not in result[method]:\n",
    "                    is_violation = result[method].get('is_violation', False)\n",
    "                    predictions.append(is_violation)\n",
    "            \n",
    "            if len(predictions) >= 2:\n",
    "                unique_predictions = len(set(predictions))\n",
    "                if unique_predictions == 1:\n",
    "                    analysis['agreement_analysis']['all_agree'] += 1\n",
    "                elif sum(predictions) > len(predictions) / 2 or sum(predictions) < len(predictions) / 2:\n",
    "                    analysis['agreement_analysis']['majority_agree'] += 1\n",
    "                else:\n",
    "                    analysis['agreement_analysis']['no_agreement'] += 1\n",
    "        \n",
    "        # Calculate percentages\n",
    "        analysis['violation_rate'] = analysis['violations_detected'] / analysis['total_reviews']\n",
    "        \n",
    "        for method in analysis['method_availability']:\n",
    "            analysis['method_availability'][method] = \\\n",
    "                analysis['method_availability'][method] / analysis['total_reviews']\n",
    "        \n",
    "        if analysis['confidence_distribution']:\n",
    "            analysis['avg_confidence'] = np.mean(analysis['confidence_distribution'])\n",
    "            analysis['confidence_std'] = np.std(analysis['confidence_distribution'])\n",
    "        \n",
    "        return analysis\n",
    "\n",
    "print(\"🔀 Hybrid Review Classifier initialized!\")\n",
    "print(\"🎯 Combines rule-based detection + traditional ML + Gemma 3 12B LLM\")\n",
    "print(\"⚖️ Configurable weights for optimal performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf9d7e2",
   "metadata": {},
   "source": [
    "## 🧪 Testing & Demonstration\n",
    "\n",
    "Now let's test our complete system with sample data and demonstrate all components working together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1585ee2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Created test dataset with 12 reviews\n",
      "📊 Violation rate: 7/12 (58.3%)\n",
      "\\n============================================================\n",
      "🔍 TEST 1: FEATURE EXTRACTION\n",
      "============================================================\n",
      "✅ Using AdvancedFeatureExtractor with loaded dataset\n",
      "🔄 Extracting features from test reviews...\n",
      "❌ Feature extraction failed: 'AdvancedFeatureExtractor' object has no attribute 'extract_features'\n",
      "\\n============================================================\n",
      "🚨 TEST 2: RULE-BASED POLICY DETECTION\n",
      "============================================================\n",
      "\\n📝 Review 1: 'The pasta was absolutely delicious and the service was outst...'\n",
      "🚨 Violation: False\n",
      "\\n📝 Review 2: 'Decent food but the service could be improved. We waited 20 ...'\n",
      "🚨 Violation: False\n",
      "\\n📝 Review 3: 'Amazing experience! The chef came out to greet us and the wi...'\n",
      "🚨 Violation: False\n",
      "\\n📝 Review 4: 'Call us now at 555-1234 for the best catering deals in town!...'\n",
      "🚨 Violation: True\n",
      "📋 Type: advertisement\n",
      "🎯 Confidence: 0.556\n",
      "\\n📝 Review 5: 'New restaurant opening! Grand opening special - 50% off all ...'\n",
      "🚨 Violation: False\n",
      "\\n============================================================\n",
      "🎯 TEST 3: TRADITIONAL ML MODELS\n",
      "============================================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'TraditionalMLPipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 109\u001b[39m\n\u001b[32m    106\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m60\u001b[39m)\n\u001b[32m    108\u001b[39m \u001b[38;5;66;03m# Initialize ML pipeline\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m ml_pipeline = \u001b[43mTraditionalMLPipeline\u001b[49m()\n\u001b[32m    111\u001b[39m \u001b[38;5;66;03m# Set feature extractor if available\u001b[39;00m\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m feature_extractor:\n",
      "\u001b[31mNameError\u001b[39m: name 'TraditionalMLPipeline' is not defined"
     ]
    }
   ],
   "source": [
    "# Create comprehensive test dataset\n",
    "test_reviews = [\n",
    "    # Legitimate reviews\n",
    "    \"The pasta was absolutely delicious and the service was outstanding. Our waiter was very attentive and the atmosphere was perfect for a romantic dinner. Highly recommend the seafood special!\",\n",
    "    \n",
    "    \"Decent food but the service could be improved. We waited 20 minutes to be seated despite having a reservation. The chicken was a bit dry but the dessert made up for it.\",\n",
    "    \n",
    "    \"Amazing experience! The chef came out to greet us and the wine selection was incredible. Will definitely be back for special occasions.\",\n",
    "    \n",
    "    # Advertisement violations\n",
    "    \"Call us now at 555-1234 for the best catering deals in town! Visit our website www.bestcatering.com for special offers. Free delivery available today only!\",\n",
    "    \n",
    "    \"New restaurant opening! Grand opening special - 50% off all meals this week. Book your table now and follow us on Instagram @newrestaurant for daily specials.\",\n",
    "    \n",
    "    # Irrelevant content violations\n",
    "    \"I hate this election season and all the political ads on TV. The weather has been terrible lately and my car broke down again. Nothing to do with restaurants but I'm frustrated.\",\n",
    "    \n",
    "    \"Just watched an amazing movie last night and can't stop thinking about it. The main actor was incredible. Also my kids are doing well in school this semester.\",\n",
    "    \n",
    "    # Rant without visit violations\n",
    "    \"I heard this place is absolutely terrible from my friends. Never been there myself but everyone says the food is disgusting and overpriced. Based on reviews, I would never go.\",\n",
    "    \n",
    "    \"Planning to visit this restaurant but the reputation seems awful. People say the service is horrible and I've read so many bad reviews online. Looks like a place to avoid.\",\n",
    "    \n",
    "    # Edge cases\n",
    "    \"Food was okay I guess. Nothing special but not terrible either.\",\n",
    "    \n",
    "    \"Best restaurant ever!!!! Amazing food amazing service amazing everything!!!!!\",\n",
    "    \n",
    "    # Mixed content\n",
    "    \"The food was great but I also want to mention that I'm selling my car. Contact me if interested. The restaurant staff was very friendly though.\"\n",
    "]\n",
    "\n",
    "# Create labels for testing (manually labeled for demonstration)\n",
    "test_labels = [\n",
    "    # Legitimate reviews (0 = no violation)\n",
    "    0, 0, 0,\n",
    "    # Advertisement violations (1 = violation)\n",
    "    1, 1,\n",
    "    # Irrelevant content violations (1 = violation)  \n",
    "    1, 1,\n",
    "    # Rant without visit violations (1 = violation)\n",
    "    1, 1,\n",
    "    # Edge cases\n",
    "    0, 0,\n",
    "    # Mixed content (1 = violation due to advertisement)\n",
    "    1\n",
    "]\n",
    "\n",
    "print(f\"🧪 Created test dataset with {len(test_reviews)} reviews\")\n",
    "print(f\"📊 Violation rate: {sum(test_labels)}/{len(test_labels)} ({sum(test_labels)/len(test_labels)*100:.1f}%)\")\n",
    "\n",
    "# Test 1: Feature Extraction\n",
    "print(\"\\\\n\" + \"=\"*60)\n",
    "print(\"🔍 TEST 1: FEATURE EXTRACTION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Initialize feature extractor if we have a dataset\n",
    "try:\n",
    "    if 'df' in locals() and not df.empty:\n",
    "        feature_extractor = AdvancedFeatureExtractor()\n",
    "        print(\"✅ Using AdvancedFeatureExtractor with loaded dataset\")\n",
    "    else:\n",
    "        print(\"⚠️ No dataset loaded, creating simple feature extractor\")\n",
    "        feature_extractor = None\n",
    "except:\n",
    "    feature_extractor = None\n",
    "\n",
    "# Create test dataframe\n",
    "test_df = pd.DataFrame({\n",
    "    'text': test_reviews,\n",
    "    'is_violation': test_labels\n",
    "})\n",
    "\n",
    "if feature_extractor:\n",
    "    try:\n",
    "        print(\"🔄 Extracting features from test reviews...\")\n",
    "        feature_df = feature_extractor.extract_features(test_df, 'text')\n",
    "        print(f\"✅ Extracted {feature_df.shape[1]} features\")\n",
    "        print(f\"📊 Feature columns: {list(feature_df.columns)[:10]}...\")  # Show first 10\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Feature extraction failed: {e}\")\n",
    "        feature_df = test_df.copy()\n",
    "else:\n",
    "    feature_df = test_df.copy()\n",
    "\n",
    "# Test 2: Rule-based Detection\n",
    "print(\"\\\\n\" + \"=\"*60)\n",
    "print(\"🚨 TEST 2: RULE-BASED POLICY DETECTION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "policy_results = []\n",
    "for i, review in enumerate(test_reviews[:5]):  # Test first 5 reviews\n",
    "    result = policy_detector.analyze_review(review)\n",
    "    policy_results.append(result)\n",
    "    \n",
    "    print(f\"\\\\n📝 Review {i+1}: '{review[:60]}...'\")\n",
    "    print(f\"🚨 Violation: {result['is_violation']}\")\n",
    "    if result['is_violation']:\n",
    "        print(f\"📋 Type: {result['violation_type']}\")\n",
    "        print(f\"🎯 Confidence: {result['confidence']:.3f}\")\n",
    "\n",
    "# Test 3: Traditional ML Models\n",
    "print(\"\\\\n\" + \"=\"*60)\n",
    "print(\"🎯 TEST 3: TRADITIONAL ML MODELS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Initialize ML pipeline\n",
    "ml_pipeline = TraditionalMLPipeline()\n",
    "\n",
    "# Set feature extractor if available\n",
    "if feature_extractor:\n",
    "    ml_pipeline.feature_extractor = feature_extractor\n",
    "\n",
    "try:\n",
    "    print(\"🚀 Training traditional ML models...\")\n",
    "    training_results = ml_pipeline.train_models(test_df, 'text', 'is_violation', test_size=0.3)\n",
    "    \n",
    "    print(\"\\\\n📊 Training Results Summary:\")\n",
    "    for model_name, result in training_results.items():\n",
    "        if 'accuracy' in result:\n",
    "            print(f\"  {model_name}: Accuracy = {result['accuracy']:.3f}\")\n",
    "    \n",
    "    # Create ensemble\n",
    "    ensemble = ml_pipeline.create_ensemble()\n",
    "    if ensemble:\n",
    "        print(\"✅ Ensemble model created successfully\")\n",
    "    \n",
    "    # Evaluate models\n",
    "    performance_df = ml_pipeline.evaluate_models(show_plots=False)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ ML training failed: {e}\")\n",
    "    print(\"🔄 Continuing with other tests...\")\n",
    "\n",
    "# Test 4: Gemma Model (if available)\n",
    "print(\"\\\\n\" + \"=\"*60)\n",
    "print(\"🤖 TEST 4: GEMMA 3 12B MODEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if gemma_classifier and gemma_classifier.client:\n",
    "    try:\n",
    "        print(\"🔄 Testing Gemma classifier with sample review...\")\n",
    "        sample_review = test_reviews[0]\n",
    "        gemma_result = gemma_classifier.classify_review(sample_review)\n",
    "        \n",
    "        print(f\"📝 Sample review: '{sample_review[:80]}...'\")\n",
    "        print(f\"🚨 Violation detected: {gemma_result.get('is_violation', 'N/A')}\")\n",
    "        print(f\"🎯 Confidence: {gemma_result.get('policy_confidence', 'N/A')}\")\n",
    "        print(f\"⭐ Quality score: {gemma_result.get('quality_score', 'N/A')}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Gemma testing failed: {e}\")\n",
    "else:\n",
    "    print(\"⚠️ Gemma classifier not available (requires HuggingFace token)\")\n",
    "\n",
    "# Test 5: Hybrid Classifier\n",
    "print(\"\\\\n\" + \"=\"*60)\n",
    "print(\"🔀 TEST 5: HYBRID CLASSIFIER\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Initialize hybrid classifier\n",
    "hybrid_classifier = HybridReviewClassifier(\n",
    "    policy_detector=policy_detector,\n",
    "    ml_pipeline=ml_pipeline,\n",
    "    gemma_classifier=gemma_classifier\n",
    ")\n",
    "\n",
    "print(\"🔄 Testing hybrid classifier on sample reviews...\")\n",
    "\n",
    "# Test on a few sample reviews\n",
    "sample_indices = [0, 3, 5, 7]  # Mix of legitimate and violation reviews\n",
    "hybrid_results = []\n",
    "\n",
    "for i in sample_indices:\n",
    "    review = test_reviews[i]\n",
    "    actual_label = test_labels[i]\n",
    "    \n",
    "    result = hybrid_classifier.classify_review(review, use_llm=False)  # Skip LLM for speed\n",
    "    hybrid_results.append(result)\n",
    "    \n",
    "    print(f\"\\\\n📝 Review {i+1}: '{review[:60]}...'\")\n",
    "    print(f\"🏷️ Actual: {'Violation' if actual_label else 'Legitimate'}\")\n",
    "    \n",
    "    final_decision = result.get('final_decision', {})\n",
    "    predicted = final_decision.get('is_violation', False)\n",
    "    confidence = final_decision.get('confidence', 0)\n",
    "    \n",
    "    print(f\"🔮 Predicted: {'Violation' if predicted else 'Legitimate'}\")\n",
    "    print(f\"🎯 Confidence: {confidence:.3f}\")\n",
    "    print(f\"✅ Correct: {predicted == bool(actual_label)}\")\n",
    "\n",
    "# Performance summary\n",
    "correct_predictions = sum(1 for i, result in enumerate(hybrid_results) \n",
    "                         if result['final_decision']['is_violation'] == bool(test_labels[sample_indices[i]]))\n",
    "\n",
    "print(f\"\\\\n📊 HYBRID CLASSIFIER PERFORMANCE:\")\n",
    "print(f\"🎯 Accuracy on test samples: {correct_predictions}/{len(hybrid_results)} ({correct_predictions/len(hybrid_results)*100:.1f}%)\")\n",
    "\n",
    "# Test 6: Batch Processing Demo\n",
    "print(\"\\\\n\" + \"=\"*60)\n",
    "print(\"📦 TEST 6: BATCH PROCESSING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"🔄 Processing all test reviews in batch...\")\n",
    "try:\n",
    "    all_results = hybrid_classifier.batch_classify(test_reviews[:8], use_llm=False, batch_size=4)\n",
    "    \n",
    "    # Analyze results\n",
    "    analysis = hybrid_classifier.analyze_performance(all_results)\n",
    "    \n",
    "    print(f\"\\\\n📊 BATCH PROCESSING RESULTS:\")\n",
    "    print(f\"📋 Total reviews processed: {analysis['total_reviews']}\")\n",
    "    print(f\"🚨 Violations detected: {analysis['violations_detected']} ({analysis['violation_rate']*100:.1f}%)\")\n",
    "    print(f\"📈 Average confidence: {analysis.get('avg_confidence', 0):.3f}\")\n",
    "    \n",
    "    print(f\"\\\\n🔧 Method availability:\")\n",
    "    for method, availability in analysis['method_availability'].items():\n",
    "        print(f\"  {method}: {availability*100:.1f}%\")\n",
    "    \n",
    "    if analysis['violation_types']:\n",
    "        print(f\"\\\\n🏷️ Violation types detected:\")\n",
    "        for vtype, count in analysis['violation_types'].items():\n",
    "            print(f\"  {vtype}: {count}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Batch processing failed: {e}\")\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*60)\n",
    "print(\"✅ TESTING COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(\"🎉 All components tested successfully!\")\n",
    "print(\"🚀 System ready for production use!\")\n",
    "print(\"💡 Next steps: Fine-tune weights, add more training data, deploy API\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1500a3",
   "metadata": {},
   "source": [
    "## 🎯 Summary & Next Steps\n",
    "\n",
    "### 🏆 What We've Built\n",
    "\n",
    "Our **Trustworthy Location Review System** includes:\n",
    "\n",
    "1. **🔍 Advanced Feature Engineering**\n",
    "   - 40+ textual and non-textual features\n",
    "   - Sentiment analysis, spam detection, readability metrics\n",
    "   - Restaurant relevancy indicators\n",
    "\n",
    "2. **🚫 Rule-Based Policy Detection**\n",
    "   - Advertisement detection (contact info, promotional language)\n",
    "   - Irrelevant content filtering (off-topic discussions)\n",
    "   - Rant without visit identification (hearsay reviews)\n",
    "\n",
    "3. **🤖 Gemma 3 12B Integration**\n",
    "   - State-of-the-art LLM for policy violation detection\n",
    "   - Quality assessment and authenticity scoring\n",
    "   - HuggingFace Inference Client integration\n",
    "\n",
    "4. **🎯 Traditional ML Pipeline**\n",
    "   - 5 different algorithms (Logistic Regression, Random Forest, SVM, etc.)\n",
    "   - Ensemble voting classifier\n",
    "   - Comprehensive evaluation metrics\n",
    "\n",
    "5. **🔀 Hybrid Ensemble Approach**\n",
    "   - Combines rule-based + ML + LLM predictions\n",
    "   - Configurable weights for optimal performance\n",
    "   - Confidence scoring and detailed reasoning\n",
    "\n",
    "### 📊 System Capabilities\n",
    "\n",
    "- **Policy Violation Detection**: Identifies advertisements, irrelevant content, and fake rants\n",
    "- **Quality Assessment**: Evaluates review authenticity and helpfulness\n",
    "- **Batch Processing**: Handles large datasets efficiently\n",
    "- **Ensemble Decision Making**: Leverages multiple approaches for robust predictions\n",
    "- **Explainable AI**: Provides reasoning for each classification decision\n",
    "\n",
    "### 🚀 Production Readiness\n",
    "\n",
    "The system is designed for:\n",
    "- **Scalability**: Batch processing with configurable sizes\n",
    "- **Reliability**: Fallback mechanisms when individual components fail\n",
    "- **Flexibility**: Adjustable weights and thresholds\n",
    "- **Monitoring**: Comprehensive performance analytics\n",
    "\n",
    "### 📈 Next Steps for Hackathon\n",
    "\n",
    "1. **🔧 Fine-Tuning**\n",
    "   - Adjust ensemble weights based on validation data\n",
    "   - Optimize decision thresholds\n",
    "   - Add domain-specific rules\n",
    "\n",
    "2. **📊 Evaluation**\n",
    "   - Test on larger datasets\n",
    "   - Measure precision, recall, F1-score\n",
    "   - A/B test different configurations\n",
    "\n",
    "3. **🚀 Deployment**\n",
    "   - Create REST API endpoints\n",
    "   - Add real-time processing capabilities\n",
    "   - Implement monitoring dashboard\n",
    "\n",
    "4. **💡 Enhancements**\n",
    "   - Add more sophisticated NLP features\n",
    "   - Implement active learning for continuous improvement\n",
    "   - Integration with restaurant platforms\n",
    "\n",
    "### 🎉 Hackathon Impact\n",
    "\n",
    "This solution addresses the critical problem of **review trustworthiness** by:\n",
    "- **Filtering noise** from restaurant review platforms\n",
    "- **Protecting consumers** from misleading information\n",
    "- **Supporting businesses** with authentic feedback\n",
    "- **Improving platform quality** through automated moderation\n",
    "\n",
    "**Ready to filter the noise and deliver trustworthy insights! 🎯**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
